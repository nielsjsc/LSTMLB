{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Data processing and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "from typing import Tuple, List, Optional, NamedTuple\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Progress tracking and logging\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Random seed\n",
    "import random\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading data from ../data/mlb_batting_data_2010_2024.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Filtering data...\n",
      "INFO:__main__:NaN counts before filtering:\n",
      "INFO:__main__:Filtered from 20503 to 5465 rows\n",
      "INFO:__main__:Created new MinMaxScaler\n",
      "INFO:__main__:Scaler data range: [-1.0000, 1.0000]\n",
      "INFO:__main__:Converting column types...\n",
      "INFO:__main__:Created 3904 valid sequences\n",
      "INFO:__main__:Skipped 0 sequences due to invalid values\n",
      "INFO:__main__:Splitting data into train, validation, and test sets\n",
      "INFO:__main__:Split sizes - Train: 2732, Valid: 780, Test: 392\n",
      "INFO:__main__:Tensor shapes - X: torch.Size([2732, 5, 30]), y: torch.Size([2732, 30]), masks: torch.Size([2732, 5])\n",
      "INFO:__main__:Value ranges - X: [-1.00, 1.00], y: [-1.00, 1.00]\n",
      "INFO:__main__:Tensor shapes - X: torch.Size([780, 5, 30]), y: torch.Size([780, 30]), masks: torch.Size([780, 5])\n",
      "INFO:__main__:Value ranges - X: [-1.00, 1.00], y: [-1.00, 1.00]\n",
      "INFO:__main__:Tensor shapes - X: torch.Size([392, 5, 30]), y: torch.Size([392, 30]), masks: torch.Size([392, 5])\n",
      "INFO:__main__:Value ranges - X: [-1.00, 1.00], y: [-1.00, 1.00]\n",
      "INFO:__main__:Created datasets - Train: 2732, Valid: 780, Test: 392\n"
     ]
    }
   ],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "   \n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    \"\"\"Configuration for data preprocessing\"\"\"\n",
    "    seq_length: int = 5\n",
    "    start_season: int = 2016\n",
    "    min_pa: int = 25\n",
    "    input_features: List[str] = None\n",
    "    train_ratio: float = 0.7\n",
    "    valid_ratio: float = 0.2\n",
    "    random_seed: int = 42\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.input_features is None:\n",
    "            self.input_features = [\n",
    "                # Base features\n",
    "                'Age', 'BB%', 'K%', 'BABIP', 'AVG', 'OBP', 'SLG', 'wOBA', 'wRC+',\n",
    "                'Barrel%', 'HardHit%', 'EV',\n",
    "                'GB%', 'FB%', 'LD%', 'Pull%', 'Oppo%', 'O-Swing%', 'Z-Swing%',\n",
    "                'Swing%', 'O-Contact%', 'Z-Contact%',\n",
    "                'Contact%', 'SwStr%', 'CSW%', 'TTO%',\n",
    "                \n",
    "                # Only include rate stats we can calculate\n",
    "                'wSB_rate', 'UBR_rate', 'wGDP_rate', 'Def_rate'\n",
    "            ]\n",
    "\n",
    "class SequenceHandler:\n",
    "    \"\"\"Handles creation and padding of sequences for LSTM input\"\"\"\n",
    "    def __init__(self, seq_length: int, feature_dim: int):\n",
    "        self.seq_length = seq_length\n",
    "        self.feature_dim = feature_dim\n",
    "        self.pad_value = 0\n",
    "        \n",
    "    def create_sequence(self, player_data: pd.DataFrame, input_features: List[str]) -> Tuple[np.ndarray, torch.Tensor]:\n",
    "        available_seasons = len(player_data)\n",
    "        \n",
    "        if available_seasons >= self.seq_length:\n",
    "            # Take most recent seasons\n",
    "            sequence = player_data.iloc[-self.seq_length:][input_features].values\n",
    "            mask = torch.ones(self.seq_length, dtype=torch.bool)\n",
    "        else:\n",
    "            # Create padding\n",
    "            padding_size = self.seq_length - available_seasons\n",
    "            real_data = player_data[input_features].values\n",
    "            padding = np.full((padding_size, len(input_features)), self.pad_value)\n",
    "            sequence = np.vstack([padding, real_data])\n",
    "            mask = torch.zeros(self.seq_length, dtype=torch.bool)\n",
    "            mask[padding_size:] = 1\n",
    "            \n",
    "        return sequence, mask\n",
    "def prepare_sequences(df: pd.DataFrame, \n",
    "                     input_features: List[str], \n",
    "                     seq_length: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Create sequences for LSTM input\"\"\"\n",
    "    sequences = []\n",
    "    masks = []\n",
    "    handler = SequenceHandler(seq_length, len(input_features))\n",
    "    \n",
    "    for _, player_data in df.groupby('IDfg'):\n",
    "        player_data = player_data.sort_values(by='Season')\n",
    "        \n",
    "        for i in range(len(player_data) - 1):\n",
    "            history = player_data.iloc[:i+1]\n",
    "            target = player_data.iloc[i+1][input_features].values\n",
    "            \n",
    "            sequence, mask = handler.create_sequence(history, input_features)\n",
    "            sequences.append((sequence, target))\n",
    "            masks.append(mask)\n",
    "    \n",
    "    return sequences, masks\n",
    "\n",
    "def calculate_rate_stats(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Calculate defensive and baserunning rate statistics using Games and PA\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Base running rates (per PA)\n",
    "    if 'wSB' in df.columns:\n",
    "        df['wSB_rate'] = df['wSB'] / df['PA']\n",
    "    if 'UBR' in df.columns:\n",
    "        df['UBR_rate'] = df['UBR'] / df['PA']\n",
    "    if 'wGDP' in df.columns:\n",
    "        df['wGDP_rate'] = df['wGDP'] / df['PA']\n",
    "    \n",
    "    # Defensive rates (per Game)\n",
    "    if 'Def' in df.columns and 'G' in df.columns:\n",
    "        df['Def_rate'] = df['Def'] / df['G']\n",
    "    \n",
    "    # Replace infinities and NaN with 0\n",
    "    rate_columns = [col for col in df.columns if col.endswith('_rate')]\n",
    "    df[rate_columns] = df[rate_columns].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def validate_features(df: pd.DataFrame, features: List[str]) -> None:\n",
    "    \"\"\"Validate that all required features exist in dataframe\"\"\"\n",
    "    missing_features = [f for f in features if f not in df.columns]\n",
    "    if missing_features:\n",
    "        raise ValueError(f\"Missing required features: {missing_features}\")\n",
    "\n",
    "def load_and_validate_data(file_path: str, config: DataConfig) -> pd.DataFrame:\n",
    "    \"\"\"Load data and perform initial validation\"\"\"\n",
    "    logger.info(f\"Loading data from {file_path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        # Calculate rate stats before validation\n",
    "        df = calculate_rate_stats(df)\n",
    "        # Then validate all features including new rate stats\n",
    "        validate_features(df, config.input_features)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {str(e)}\")\n",
    "        raise\n",
    "def split_data(sequences: List[Tuple], \n",
    "               masks: List[torch.Tensor],\n",
    "               train_ratio: float = 0.7,\n",
    "               valid_ratio: float = 0.2) -> Tuple:\n",
    "    \"\"\"Split data into train, validation and test sets\"\"\"\n",
    "    # Validate ratios\n",
    "    if not 0 < train_ratio + valid_ratio < 1:\n",
    "        raise ValueError(\"Train and validation ratios must sum to less than 1\")\n",
    "    \n",
    "    logger.info(\"Splitting data into train, validation, and test sets\")\n",
    "    n = len(sequences)\n",
    "    indices = np.random.permutation(n)\n",
    "    \n",
    "    train_size = int(n * train_ratio)\n",
    "    valid_size = int(n * valid_ratio)\n",
    "    \n",
    "    train_indices = indices[:train_size]\n",
    "    valid_indices = indices[train_size:train_size + valid_size]\n",
    "    test_indices = indices[train_size + valid_size:]\n",
    "    \n",
    "    # Split sequences and masks\n",
    "    train_data = ([sequences[i] for i in train_indices], [masks[i] for i in train_indices])\n",
    "    valid_data = ([sequences[i] for i in valid_indices], [masks[i] for i in valid_indices])\n",
    "    test_data = ([sequences[i] for i in test_indices], [masks[i] for i in test_indices])\n",
    "    \n",
    "    logger.info(f\"Split sizes - Train: {len(train_indices)}, Valid: {len(valid_indices)}, Test: {len(test_indices)}\")\n",
    "    \n",
    "    return train_data, valid_data, test_data\n",
    "def filter_data(df: pd.DataFrame, config: DataConfig) -> pd.DataFrame:\n",
    "    \"\"\"Filter data based on configuration\"\"\"\n",
    "    logger.info(\"Filtering data...\")\n",
    "    initial_size = len(df)\n",
    "    \n",
    "    # Filter by season\n",
    "    df = df[df['Season'] >= config.start_season]\n",
    "    \n",
    "    # Filter by minimum PA\n",
    "    df = df[df['PA'] >= config.min_pa]\n",
    "    # Calculate rate statistics\n",
    "    df = calculate_rate_stats(df)\n",
    "    \n",
    "    # Drop NaN values in input features early\n",
    "    df = df.dropna(subset=config.input_features)\n",
    "    \n",
    "    # Log statistics before filtering\n",
    "    logger.info(\"NaN counts before filtering:\")\n",
    "    for col in config.input_features:\n",
    "        nan_count = df[col].isna().sum()\n",
    "        if nan_count > 0:\n",
    "            logger.warning(f\"{col}: {nan_count} NaN values\")\n",
    "            \n",
    "    logger.info(f\"Filtered from {initial_size} to {len(df)} rows\")\n",
    "    return df\n",
    "\n",
    "def convert_column_types(df: pd.DataFrame, features: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Convert columns to float32 for LSTM compatibility\"\"\"\n",
    "    logger.info(\"Converting column types...\")\n",
    "    \n",
    "    for col in features:\n",
    "        try:\n",
    "            # Convert percentage strings to floats if needed\n",
    "            if df[col].dtype == object and df[col].str.contains('%').any():\n",
    "                df[col] = df[col].str.rstrip('%').astype('float32') / 100\n",
    "            else:\n",
    "                df[col] = df[col].astype('float32')\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error converting column {col}: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    return df\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def scale_features(df: pd.DataFrame, \n",
    "                  features: List[str], \n",
    "                  scaler: Optional[MinMaxScaler] = None) -> Tuple[pd.DataFrame, MinMaxScaler]:\n",
    "    \"\"\"Scale features using MinMaxScaler and add player-specific normalized features\"\"\"\n",
    "    \n",
    "    # Calculate player career averages\n",
    "    player_stats = df.groupby('IDfg')[features].transform('mean')\n",
    "    \n",
    "    # Create deviation from career average features\n",
    "    for feature in features:\n",
    "        df[f'{feature}_vs_career'] = df[feature] - player_stats[feature]\n",
    "    \n",
    "    # Combine original and new features\n",
    "    all_features = features + [f'{feature}_vs_career' for feature in features]\n",
    "    \n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1))  # Use (-1,1) range for better neural network training\n",
    "        scaled_data = scaler.fit_transform(df[all_features])\n",
    "        joblib.dump(scaler, 'batter_scaler.pkl')\n",
    "        logger.info(f\"Created new MinMaxScaler\")\n",
    "        logger.info(f\"Scaler data range: [{scaled_data.min():.4f}, {scaled_data.max():.4f}]\")\n",
    "    else:\n",
    "        scaled_data = scaler.transform(df[all_features])\n",
    "    \n",
    "    # Validate scaled data\n",
    "    if np.isnan(scaled_data).any():\n",
    "        raise ValueError(\"NaN values found after scaling\")\n",
    "    if np.isinf(scaled_data).any():\n",
    "        raise ValueError(\"Infinite values found after scaling\")\n",
    "    \n",
    "    # Update DataFrame with scaled values\n",
    "    scaled_df = pd.DataFrame(scaled_data, columns=all_features, index=df.index)\n",
    "    df[all_features] = scaled_df\n",
    "    \n",
    "    return df, scaler\n",
    "\n",
    "def prepare_sequences(df: pd.DataFrame, \n",
    "                      input_features: List[str],\n",
    "                      seq_length: int) -> Tuple[List, List]:\n",
    "    \"\"\"Create sequences for LSTM input with padding for shorter histories.\"\"\"\n",
    "    sequences = []\n",
    "    masks = []\n",
    "    handler = SequenceHandler(seq_length, len(input_features))\n",
    "    skipped_sequences = 0\n",
    "\n",
    "    # Convert types before processing\n",
    "    df = convert_column_types(df, input_features)\n",
    "\n",
    "    # Sort by 'IDfg' and 'Season' to ensure correct order\n",
    "    df = df.sort_values(['IDfg', 'Season'])\n",
    "\n",
    "    for player_id, player_data in df.groupby('IDfg'):\n",
    "        player_data = player_data.reset_index(drop=True)\n",
    "        num_seasons = len(player_data)\n",
    "\n",
    "        if num_seasons < 2:\n",
    "            continue  # Need at least two seasons to create a sequence\n",
    "\n",
    "        # Generate sequences starting from the second season\n",
    "        for i in range(1, num_seasons):\n",
    "            history = player_data.iloc[:i]\n",
    "            target = player_data.iloc[i][input_features].values\n",
    "\n",
    "            if history[input_features].isna().any().any() or pd.isna(target).any():\n",
    "                skipped_sequences += 1\n",
    "                continue\n",
    "\n",
    "            sequence, mask = handler.create_sequence(history, input_features)\n",
    "\n",
    "            if np.isnan(sequence).any() or np.isinf(sequence).any():\n",
    "                skipped_sequences += 1\n",
    "                continue\n",
    "\n",
    "            sequences.append((sequence, target))\n",
    "            masks.append(mask)\n",
    "\n",
    "    logger.info(f\"Created {len(sequences)} valid sequences\")\n",
    "    logger.info(f\"Skipped {skipped_sequences} sequences due to invalid values\")\n",
    "\n",
    "    if not sequences:\n",
    "        raise ValueError(\"No valid sequences created after filtering\")\n",
    "\n",
    "    return sequences, masks\n",
    "\n",
    "def to_tensor(sequences: List[Tuple], masks: List[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Convert sequences and masks to PyTorch tensors with validation\"\"\"\n",
    "    sequences_array = np.array([s[0] for s in sequences], dtype=np.float32)\n",
    "    targets_array = np.array([s[1] for s in sequences], dtype=np.float32)\n",
    "    \n",
    "    # Validate arrays before conversion\n",
    "    if np.isnan(sequences_array).any():\n",
    "        raise ValueError(\"NaN values found in input sequences\")\n",
    "    if np.isnan(targets_array).any():\n",
    "        raise ValueError(\"NaN values found in target values\")\n",
    "        \n",
    "    X = torch.FloatTensor(sequences_array)\n",
    "    y = torch.FloatTensor(targets_array)\n",
    "    masks = torch.stack(masks)\n",
    "    \n",
    "    logger.info(f\"Tensor shapes - X: {X.shape}, y: {y.shape}, masks: {masks.shape}\")\n",
    "    logger.info(f\"Value ranges - X: [{X.min():.2f}, {X.max():.2f}], y: [{y.min():.2f}, {y.max():.2f}]\")\n",
    "    \n",
    "    return X, y, masks\n",
    "\n",
    "def preprocess_data(file_path: str, config: DataConfig) -> Tuple:\n",
    "    \"\"\"Main preprocessing function with enhanced validation\"\"\"\n",
    "    try:\n",
    "        # Set random seeds\n",
    "        torch.manual_seed(config.random_seed)\n",
    "        np.random.seed(config.random_seed)\n",
    "        \n",
    "        # Load and validate raw data\n",
    "        df = load_and_validate_data(file_path, config)\n",
    "        \n",
    "        # Filter and clean data\n",
    "        df = filter_data(df, config)\n",
    "        \n",
    "        # Scale features before sequence creation\n",
    "        df, scaler = scale_features(df, config.input_features)\n",
    "        \n",
    "        # Create sequences\n",
    "        sequences, masks = prepare_sequences(df, config.input_features, config.seq_length)\n",
    "        \n",
    "        # Split data\n",
    "        train_data, valid_data, test_data = split_data(sequences, masks, \n",
    "                                                      train_ratio=config.train_ratio,\n",
    "                                                      valid_ratio=config.valid_ratio)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        X_train, y_train, train_masks = to_tensor(*train_data)\n",
    "        X_valid, y_valid, valid_masks = to_tensor(*valid_data)\n",
    "        X_test, y_test, test_masks = to_tensor(*test_data)\n",
    "        \n",
    "        logger.info(f\"Created datasets - Train: {len(X_train)}, Valid: {len(X_valid)}, Test: {len(X_test)}\")\n",
    "        \n",
    "        return (X_train, y_train, X_valid, y_valid, X_test, y_test, \n",
    "                train_masks, valid_masks, test_masks)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in preprocessing: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Initialize configuration with all features\n",
    "data_config = DataConfig(\n",
    "    input_features=[\n",
    "        # Base features\n",
    "        'Age', 'BB%', 'K%', 'BABIP', 'AVG', 'OBP', 'SLG', 'wOBA', 'wRC+',\n",
    "        'Barrel%', 'HardHit%', 'EV',\n",
    "        'GB%', 'FB%', 'LD%', 'Pull%', 'Oppo%', 'O-Swing%', 'Z-Swing%',\n",
    "        'Swing%', 'O-Contact%', 'Z-Contact%',\n",
    "        'Contact%', 'SwStr%', 'CSW%', 'TTO%',\n",
    "        \n",
    "        # New rate stats\n",
    "        'wSB_rate', 'UBR_rate','wGDP_rate', 'Def_rate'\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "data = preprocess_data('../data/mlb_batting_data_2010_2024.csv', data_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        hidden_size: int,\n",
    "        num_heads: int = 8,\n",
    "        dropout: float = 0.1,\n",
    "        bias: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        self.scaling = self.head_dim ** -0.5\n",
    "        \n",
    "        assert self.head_dim * num_heads == hidden_size, \"hidden_size must be divisible by num_heads\"\n",
    "        \n",
    "        # Linear projections\n",
    "        self.q_proj = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "        self.k_proj = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "        self.v_proj = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "        self.out_proj = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self._reset_parameters()\n",
    "    \n",
    "    def _reset_parameters(self):\n",
    "        # Use Xavier uniform initialization\n",
    "        nn.init.xavier_uniform_(self.q_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.k_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.v_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.out_proj.weight)\n",
    "        if self.q_proj.bias is not None:\n",
    "            nn.init.zeros_(self.q_proj.bias)\n",
    "            nn.init.zeros_(self.k_proj.bias)\n",
    "            nn.init.zeros_(self.v_proj.bias)\n",
    "            nn.init.zeros_(self.out_proj.bias)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: Optional[torch.Tensor] = None,\n",
    "        value: Optional[torch.Tensor] = None,\n",
    "        key_padding_mask: Optional[torch.Tensor] = None,\n",
    "        need_weights: bool = False\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        # Set key and value to query if not provided\n",
    "        if key is None:\n",
    "            key = query\n",
    "        if value is None:\n",
    "            value = query\n",
    "            \n",
    "        batch_size, seq_len, _ = query.size()\n",
    "        \n",
    "        # Project inputs\n",
    "        q = self.q_proj(query)\n",
    "        k = self.k_proj(key)\n",
    "        v = self.v_proj(value)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attn_weights = torch.matmul(q, k.transpose(-2, -1)) * self.scaling\n",
    "        \n",
    "        # Apply key padding mask if provided\n",
    "        if key_padding_mask is not None:\n",
    "            attn_weights = attn_weights.masked_fill(\n",
    "                key_padding_mask.unsqueeze(1).unsqueeze(2),\n",
    "                float('-inf')\n",
    "            )\n",
    "        \n",
    "        # Apply softmax and dropout\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Get attention output\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        # Reshape and project output\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.view(batch_size, seq_len, self.hidden_size)\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        \n",
    "        if need_weights:\n",
    "            return attn_output, attn_weights\n",
    "        return attn_output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, hidden_size: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size * 4, hidden_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.layers(self.layer_norm(x))\n",
    "\n",
    "class ImprovedLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size: int,\n",
    "        hidden_size: int = 512,\n",
    "        num_layers: int = 4,\n",
    "        output_size: int = None,\n",
    "        dropout: float = 0.3,\n",
    "        bidirectional: bool = True,\n",
    "        num_heads: int = 8  # Keep this parameter for compatibility\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size // 2  # Reduce internal hidden size\n",
    "        self.num_layers = 2  # Use fewer layers internally\n",
    "        self.output_size = output_size or input_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.directions = 2 if bidirectional else 1\n",
    "        \n",
    "        # Learned padding token\n",
    "        self.pad_token = nn.Parameter(torch.randn(1, 1, input_size))\n",
    "        \n",
    "        # Input projection with Layer Normalization\n",
    "        self.input_projection = nn.Sequential(\n",
    "            nn.Linear(input_size, self.hidden_size),\n",
    "            nn.LayerNorm(self.hidden_size),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # LSTM layers with residual connections and layer normalization\n",
    "        self.lstm_layers = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                'lstm': nn.LSTM(\n",
    "                    self.hidden_size * self.directions if i > 0 else self.hidden_size,\n",
    "                    self.hidden_size,\n",
    "                    num_layers=1,\n",
    "                    batch_first=True,\n",
    "                    bidirectional=bidirectional\n",
    "                ),\n",
    "                'norm': nn.LayerNorm(self.hidden_size * self.directions),\n",
    "                'dropout': nn.Dropout(dropout/2)  # Reduce dropout\n",
    "            }) for i in range(self.num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = MultiHeadAttention(\n",
    "            self.hidden_size * self.directions,\n",
    "            num_heads=4,  # Reduced from num_heads parameter\n",
    "            dropout=dropout/2\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size * self.directions, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout/2),\n",
    "            nn.Linear(hidden_size, self.output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # Replace zero padding with learned padding token\n",
    "        padding_mask = (x.sum(dim=-1) == 0).unsqueeze(-1)\n",
    "        x = torch.where(padding_mask, self.pad_token.expand(batch_size, seq_len, -1), x)\n",
    "        \n",
    "        # Create attention mask\n",
    "        attention_mask = torch.arange(seq_len, device=x.device)[None, :] < lengths[:, None]\n",
    "        \n",
    "        # Input projection\n",
    "        x = self.input_projection(x)\n",
    "        \n",
    "        # Process LSTM layers with residual connections\n",
    "        for layer in self.lstm_layers:\n",
    "            # Pack padded sequence\n",
    "            packed_x = pack_padded_sequence(\n",
    "                x, lengths.cpu(),\n",
    "                batch_first=True,\n",
    "                enforce_sorted=False\n",
    "            )\n",
    "            \n",
    "            # LSTM forward pass\n",
    "            lstm_out, _ = layer['lstm'](packed_x)\n",
    "            lstm_out, _ = pad_packed_sequence(\n",
    "                lstm_out,\n",
    "                batch_first=True,\n",
    "                total_length=seq_len\n",
    "            )\n",
    "            \n",
    "            # Apply normalization and dropout\n",
    "            lstm_out = layer['norm'](lstm_out)\n",
    "            lstm_out = layer['dropout'](lstm_out)\n",
    "            \n",
    "            # Residual connection if shapes match\n",
    "            if lstm_out.size(-1) == x.size(-1):\n",
    "                x = x + lstm_out\n",
    "            else:\n",
    "                x = lstm_out\n",
    "        \n",
    "        # Apply attention with proper masking\n",
    "        attended, _ = self.attention(\n",
    "            x, x, x,\n",
    "            key_padding_mask=~attention_mask\n",
    "        )\n",
    "        \n",
    "        # Get final states using sequence lengths\n",
    "        batch_indices = torch.arange(batch_size, device=x.device)\n",
    "        final_states = attended[batch_indices, lengths - 1]\n",
    "        \n",
    "        # Project to output size\n",
    "        output = self.output_projection(final_states)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Model Configuration:\n",
      "INFO:__main__:input_size: 30\n",
      "INFO:__main__:output_size: 30\n",
      "INFO:__main__:hidden_size: 256\n",
      "INFO:__main__:num_layers: 4\n",
      "INFO:__main__:num_heads: 8\n",
      "INFO:__main__:bidirectional: True\n",
      "INFO:__main__:attention_dropout: 0.1\n",
      "INFO:__main__:residual_dropout: 0.2\n",
      "INFO:__main__:layer_norm_eps: 1e-05\n",
      "INFO:__main__:batch_size: 16\n",
      "INFO:__main__:dropout: 0.3\n",
      "INFO:__main__:learning_rate: 0.001\n",
      "INFO:__main__:weight_decay: 1e-05\n",
      "INFO:__main__:gradient_clip: 1.0\n",
      "INFO:__main__:num_epochs: 50\n",
      "INFO:__main__:warmup_epochs: 5\n",
      "INFO:__main__:lr_schedule: cosine\n",
      "INFO:__main__:min_lr: 1e-06\n",
      "INFO:__main__:lr_decay_rate: 0.1\n",
      "INFO:__main__:lr_patience: 5\n",
      "INFO:__main__:early_stopping_patience: 10\n",
      "INFO:__main__:early_stopping_min_delta: 0.0001\n",
      "INFO:__main__:diversity_alpha: 0.1\n",
      "INFO:__main__:consistency_beta: 0.05\n",
      "INFO:__main__:mixed_precision: True\n",
      "INFO:__main__:num_workers: 0\n",
      "INFO:__main__:pin_memory: True\n",
      "INFO:__main__:log_interval: 100\n",
      "INFO:__main__:checkpoint_interval: 1\n",
      "INFO:__main__:Data loaded successfully. Device: cuda\n",
      "INFO:__main__:Model initialized with 1001916 parameters\n",
      "INFO:__main__:Training batches: 171\n",
      "INFO:__main__:Validation batches: 49\n",
      "INFO:__main__:Test batches: 25\n"
     ]
    }
   ],
   "source": [
    "#Model configuration\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DataBatch(NamedTuple):\n",
    "    \"\"\"Container for training data batches.\"\"\"\n",
    "    train: TensorDataset\n",
    "    valid: TensorDataset\n",
    "    test: TensorDataset\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Advanced configuration for LSTM-based baseball statistics prediction.\"\"\"\n",
    "    \n",
    "    # Dynamic sizes from data\n",
    "    input_size: int = None\n",
    "    output_size: int = None\n",
    "    \n",
    "    # Model Architecture \n",
    "    hidden_size: int = 256\n",
    "    num_layers: int = 4\n",
    "    num_heads: int = 8\n",
    "    bidirectional: bool = True\n",
    "    attention_dropout: float = 0.1\n",
    "    residual_dropout: float = 0.2\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    \n",
    "    # Training Parameters\n",
    "    batch_size: int = 16\n",
    "    dropout: float = 0.3\n",
    "    learning_rate: float = 1e-3\n",
    "    weight_decay: float = 1e-5\n",
    "    gradient_clip: float = 1.0\n",
    "    num_epochs: int = 50\n",
    "    warmup_epochs: int = 5\n",
    "    \n",
    "    # Learning Rate Schedule\n",
    "    lr_schedule: str = 'cosine'\n",
    "    min_lr: float = 1e-6\n",
    "    lr_decay_rate: float = 0.1\n",
    "    lr_patience: int = 5\n",
    "    \n",
    "    # Early Stopping\n",
    "    early_stopping_patience: int = 10\n",
    "    early_stopping_min_delta: float = 1e-4\n",
    "    \n",
    "    # Loss Function Parameters\n",
    "    diversity_alpha: float = 0.1  # Weight for diversity penalty\n",
    "    consistency_beta: float = 0.05  # Weight for consistency penalty\n",
    "    \n",
    "    # Hardware Optimization\n",
    "    mixed_precision: bool = True\n",
    "    num_workers: int = 0\n",
    "    pin_memory: bool = True\n",
    "    \n",
    "    # Logging\n",
    "    log_interval: int = 100\n",
    "    checkpoint_interval: int = 1\n",
    "    \n",
    "    def __init__(self, X_train: torch.Tensor, y_train: torch.Tensor):\n",
    "        self.input_size = X_train.shape[2]\n",
    "        self.output_size = y_train.shape[1]\n",
    "        self._validate_config()\n",
    "        self._log_config()\n",
    "    \n",
    "    def _validate_config(self) -> None:\n",
    "        assert self.hidden_size % self.num_heads == 0, \\\n",
    "            \"Hidden size must be divisible by number of attention heads\"\n",
    "        assert self.hidden_size >= self.input_size, \\\n",
    "            \"Hidden size must be greater than or equal to input size\"\n",
    "        assert 0 <= self.dropout <= 1, \"Dropout must be between 0 and 1\"\n",
    "        assert self.num_layers >= 1, \"Must have at least one LSTM layer\"\n",
    "        assert self.batch_size > 0, \"Batch size must be positive\"\n",
    "        assert self.learning_rate > 0, \"Learning rate must be positive\"\n",
    "        assert self.lr_schedule in ['cosine', 'linear', 'exponential'], \\\n",
    "            \"Invalid learning rate schedule\"\n",
    "        assert 0 <= self.diversity_alpha <= 1, \"Diversity alpha must be between 0 and 1\"\n",
    "        assert 0 <= self.consistency_beta <= 1, \"Consistency beta must be between 0 and 1\"\n",
    "    \n",
    "    def _log_config(self) -> None:\n",
    "        logger.info(\"Model Configuration:\")\n",
    "        for key, value in asdict(self).items():\n",
    "            logger.info(f\"{key}: {value}\")\n",
    "    \n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class PlayerDifferentiationLoss(nn.Module):\n",
    "    def __init__(self, alpha: float = 0.1, beta: float = 0.05):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        # Base MSE loss\n",
    "        mse_loss = self.mse(predictions, targets)\n",
    "        \n",
    "        # Diversity penalty - encourage different predictions within batch\n",
    "        batch_mean = predictions.mean(dim=0, keepdim=True)\n",
    "        diversity_loss = -torch.mean(torch.abs(predictions - batch_mean))\n",
    "        \n",
    "        # Consistency penalty - predictions should be stable\n",
    "        pred_std = predictions.std(dim=0).mean()\n",
    "        consistency_loss = torch.abs(pred_std - targets.std(dim=0).mean())\n",
    "        \n",
    "        # Combine losses\n",
    "        total_loss = mse_loss + self.alpha * diversity_loss + self.beta * consistency_loss\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        \"\"\"Get appropriate device for training.\"\"\"\n",
    "        return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def create_data_loaders(data_tuple: tuple) -> DataBatch:\n",
    "    \"\"\"Create DataLoader objects from preprocessed data tuple.\"\"\"\n",
    "    try:\n",
    "        X_train, y_train, X_valid, y_valid, X_test, y_test, \\\n",
    "        train_masks, valid_masks, test_masks = data_tuple\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = TensorDataset(X_train, train_masks, y_train)\n",
    "        valid_dataset = TensorDataset(X_valid, valid_masks, y_valid)\n",
    "        test_dataset = TensorDataset(X_test, test_masks, y_test)\n",
    "        \n",
    "        return DataBatch(train_dataset, valid_dataset, test_dataset)\n",
    "    \n",
    "    except ValueError as e:\n",
    "        logger.error(f\"Error unpacking data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Initialize everything\n",
    "try:\n",
    "    # Create data loaders\n",
    "    data_batch = create_data_loaders(data)\n",
    "    \n",
    "    # Initialize config\n",
    "    config = Config(data_batch.train.tensors[0], data_batch.train.tensors[2])\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        data_batch.train,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=config.pin_memory\n",
    "    )\n",
    "    \n",
    "    valid_loader = DataLoader(\n",
    "        data_batch.valid,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=config.pin_memory\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        data_batch.test,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=config.pin_memory\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = ImprovedLSTM(\n",
    "        input_size=config.input_size,\n",
    "        hidden_size=config.hidden_size,\n",
    "        num_layers=config.num_layers,\n",
    "        output_size=config.output_size,\n",
    "        dropout=config.dropout,\n",
    "        bidirectional=config.bidirectional,\n",
    "        num_heads=config.num_heads\n",
    "    ).to(config.device)\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config.learning_rate,\n",
    "        weight_decay=config.weight_decay\n",
    "    )\n",
    "    \n",
    "    # Initialize learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=config.learning_rate,\n",
    "        epochs=config.num_epochs,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        pct_start=config.warmup_epochs / config.num_epochs,\n",
    "        anneal_strategy='cos',\n",
    "        final_div_factor=1e3\n",
    "    )\n",
    "    \n",
    "    # Initialize loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    logger.info(f\"Data loaded successfully. Device: {config.device}\")\n",
    "    logger.info(f\"Model initialized with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "    logger.info(f\"Training batches: {len(train_loader)}\")\n",
    "    logger.info(f\"Validation batches: {len(valid_loader)}\")\n",
    "    logger.info(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during initialization: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting training on device: cuda\n",
      "Epoch 1/50:   0%|          | 0/171 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Batch shapes - Data: torch.Size([16, 5, 30]), Masks: torch.Size([16, 5]), Targets: torch.Size([16, 30]), Lengths: torch.Size([16])\n",
      "INFO:__main__:Output shape: torch.Size([16, 30])\n",
      "Epoch 1/50: 100%|██████████| 171/171 [00:05<00:00, 33.55it/s, loss=0.050, lr=1.32e-04]\n",
      "INFO:__main__:New best model saved with validation loss: 0.0434\n",
      "INFO:__main__:Epoch 1: Train Loss = 0.0886, Val Loss = 0.0434, LR = 1.32e-04\n",
      "Epoch 2/50: 100%|██████████| 171/171 [00:05<00:00, 29.98it/s, loss=0.067, lr=3.72e-04]\n",
      "INFO:__main__:New best model saved with validation loss: 0.0408\n",
      "INFO:__main__:Epoch 2: Train Loss = 0.0528, Val Loss = 0.0408, LR = 3.72e-04\n",
      "Epoch 3/50: 100%|██████████| 171/171 [00:05<00:00, 29.86it/s, loss=0.048, lr=6.69e-04]\n",
      "INFO:__main__:New best model saved with validation loss: 0.0394\n",
      "INFO:__main__:Epoch 3: Train Loss = 0.0466, Val Loss = 0.0394, LR = 6.69e-04\n",
      "Epoch 4/50: 100%|██████████| 171/171 [00:05<00:00, 33.29it/s, loss=0.047, lr=9.09e-04]\n",
      "INFO:__main__:Epoch 4: Train Loss = 0.0444, Val Loss = 0.0409, LR = 9.09e-04\n",
      "Epoch 5/50: 100%|██████████| 171/171 [00:05<00:00, 33.15it/s, loss=0.059, lr=1.00e-03]\n",
      "INFO:__main__:New best model saved with validation loss: 0.0384\n",
      "INFO:__main__:Epoch 5: Train Loss = 0.0434, Val Loss = 0.0384, LR = 1.00e-03\n",
      "Epoch 6/50: 100%|██████████| 171/171 [00:05<00:00, 32.42it/s, loss=0.035, lr=9.99e-04]\n",
      "INFO:__main__:Epoch 6: Train Loss = 0.0427, Val Loss = 0.0387, LR = 9.99e-04\n",
      "Epoch 7/50: 100%|██████████| 171/171 [00:05<00:00, 31.48it/s, loss=0.032, lr=9.95e-04]\n",
      "INFO:__main__:Epoch 7: Train Loss = 0.0418, Val Loss = 0.0393, LR = 9.95e-04\n",
      "Epoch 8/50: 100%|██████████| 171/171 [00:05<00:00, 30.83it/s, loss=0.025, lr=9.89e-04]\n",
      "INFO:__main__:New best model saved with validation loss: 0.0380\n",
      "INFO:__main__:Epoch 8: Train Loss = 0.0414, Val Loss = 0.0380, LR = 9.89e-04\n",
      "Epoch 9/50: 100%|██████████| 171/171 [00:05<00:00, 30.35it/s, loss=0.050, lr=9.81e-04]\n",
      "INFO:__main__:Epoch 9: Train Loss = 0.0411, Val Loss = 0.0382, LR = 9.81e-04\n",
      "Epoch 10/50: 100%|██████████| 171/171 [00:05<00:00, 33.94it/s, loss=0.043, lr=9.70e-04]\n",
      "INFO:__main__:Epoch 10: Train Loss = 0.0410, Val Loss = 0.0384, LR = 9.70e-04\n",
      "Epoch 11/50: 100%|██████████| 171/171 [00:05<00:00, 33.58it/s, loss=0.059, lr=9.57e-04]\n",
      "INFO:__main__:New best model saved with validation loss: 0.0378\n",
      "INFO:__main__:Epoch 11: Train Loss = 0.0408, Val Loss = 0.0378, LR = 9.57e-04\n",
      "Epoch 12/50: 100%|██████████| 171/171 [00:05<00:00, 28.85it/s, loss=0.024, lr=9.41e-04]\n",
      "INFO:__main__:Epoch 12: Train Loss = 0.0405, Val Loss = 0.0392, LR = 9.41e-04\n",
      "Epoch 13/50: 100%|██████████| 171/171 [00:05<00:00, 29.83it/s, loss=0.031, lr=9.24e-04]\n",
      "INFO:__main__:Epoch 13: Train Loss = 0.0405, Val Loss = 0.0388, LR = 9.24e-04\n",
      "Epoch 14/50: 100%|██████████| 171/171 [00:05<00:00, 33.69it/s, loss=0.044, lr=9.04e-04]\n",
      "INFO:__main__:Epoch 14: Train Loss = 0.0401, Val Loss = 0.0388, LR = 9.04e-04\n",
      "Epoch 15/50: 100%|██████████| 171/171 [00:05<00:00, 32.91it/s, loss=0.034, lr=8.83e-04]\n",
      "INFO:__main__:Epoch 15: Train Loss = 0.0399, Val Loss = 0.0392, LR = 8.83e-04\n",
      "Epoch 16/50: 100%|██████████| 171/171 [00:05<00:00, 30.78it/s, loss=0.043, lr=8.60e-04]\n",
      "INFO:__main__:Epoch 16: Train Loss = 0.0398, Val Loss = 0.0394, LR = 8.60e-04\n",
      "Epoch 17/50: 100%|██████████| 171/171 [00:05<00:00, 30.11it/s, loss=0.050, lr=8.34e-04]\n",
      "INFO:__main__:Epoch 17: Train Loss = 0.0394, Val Loss = 0.0385, LR = 8.34e-04\n",
      "Epoch 18/50: 100%|██████████| 171/171 [00:07<00:00, 21.68it/s, loss=0.024, lr=8.08e-04]\n",
      "INFO:__main__:Epoch 18: Train Loss = 0.0392, Val Loss = 0.0394, LR = 8.08e-04\n",
      "Epoch 19/50: 100%|██████████| 171/171 [00:07<00:00, 23.03it/s, loss=0.035, lr=7.79e-04]\n",
      "INFO:__main__:Epoch 19: Train Loss = 0.0390, Val Loss = 0.0385, LR = 7.79e-04\n",
      "Epoch 20/50: 100%|██████████| 171/171 [00:06<00:00, 26.18it/s, loss=0.021, lr=7.50e-04]\n",
      "INFO:__main__:Epoch 20: Train Loss = 0.0389, Val Loss = 0.0380, LR = 7.50e-04\n",
      "Epoch 21/50: 100%|██████████| 171/171 [00:07<00:00, 22.00it/s, loss=0.041, lr=7.19e-04]\n",
      "INFO:__main__:Epoch 21: Train Loss = 0.0386, Val Loss = 0.0382, LR = 7.19e-04\n",
      "INFO:__main__:Early stopping triggered after 21 epochs\n"
     ]
    }
   ],
   "source": [
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    valid_loader: DataLoader,\n",
    "    config: Config,\n",
    "    optimizer: optim.Optimizer,\n",
    "    scheduler: optim.lr_scheduler._LRScheduler,\n",
    "    criterion: nn.Module,\n",
    "    checkpoint_dir: str = './checkpoints'\n",
    ") -> dict:\n",
    "    \"\"\"Train LSTM model with advanced optimizations and monitoring.\"\"\"\n",
    "    import os\n",
    "    \n",
    "    logger.info(f\"Starting training on device: {config.device}\")\n",
    "    model = model.to(config.device)\n",
    "    \n",
    "    # Mixed precision training\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=config.mixed_precision)\n",
    "    \n",
    "    # Training state tracking\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    train_metrics = {\n",
    "        'train_losses': [],\n",
    "        'val_losses': [],\n",
    "        'learning_rates': [],\n",
    "        'best_epoch': 0\n",
    "    }\n",
    "    \n",
    "    # Create checkpoint directory\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        with tqdm(train_loader, desc=f'Epoch {epoch+1}/{config.num_epochs}') as pbar:\n",
    "            for batch_idx, (data, masks, targets) in enumerate(pbar):\n",
    "                try:\n",
    "                    # Move data to device\n",
    "                    data = data.to(config.device)\n",
    "                    masks = masks.to(config.device)\n",
    "                    targets = targets.to(config.device)\n",
    "                    \n",
    "                    # Calculate sequence lengths from masks\n",
    "                    lengths = masks.sum(1).clamp(min=1)  # Ensure minimum length of 1\n",
    "                    \n",
    "                    # Debug info\n",
    "                    if batch_idx == 0 and epoch == 0:\n",
    "                        logger.info(f\"Batch shapes - Data: {data.shape}, Masks: {masks.shape}, \"\n",
    "                                  f\"Targets: {targets.shape}, Lengths: {lengths.shape}\")\n",
    "                    \n",
    "                    # Forward pass with mixed precision\n",
    "                    with torch.cuda.amp.autocast(enabled=config.mixed_precision):\n",
    "                        outputs = model(data, lengths)\n",
    "                        if batch_idx == 0 and epoch == 0:\n",
    "                            logger.info(f\"Output shape: {outputs.shape}\")\n",
    "                        loss = criterion(outputs, targets)\n",
    "                    \n",
    "                    # Backward pass with gradient scaling\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    scaler.scale(loss).backward()\n",
    "                    \n",
    "                    # Gradient clipping\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.gradient_clip)\n",
    "                    \n",
    "                    # Optimizer step with scaler\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    scheduler.step()\n",
    "                    \n",
    "                    # Update metrics\n",
    "                    epoch_loss += loss.item()\n",
    "                    current_lr = scheduler.get_last_lr()[0]\n",
    "                    \n",
    "                    # Update progress bar\n",
    "                    pbar.set_postfix({\n",
    "                        'loss': f'{loss.item():.3f}',\n",
    "                        'lr': f'{current_lr:.2e}'\n",
    "                    })\n",
    "                    \n",
    "                except RuntimeError as e:\n",
    "                    logger.error(f\"Error in batch {batch_idx}: {str(e)}\")\n",
    "                    logger.error(f\"Data shapes - Input: {data.shape}, Mask: {masks.shape}, \"\n",
    "                               f\"Target: {targets.shape}, Lengths: {lengths.shape}\")\n",
    "                    raise\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, masks, targets in valid_loader:\n",
    "                try:\n",
    "                    data = data.to(config.device)\n",
    "                    masks = masks.to(config.device)\n",
    "                    targets = targets.to(config.device)\n",
    "                    lengths = masks.sum(1).clamp(min=1)\n",
    "                    \n",
    "                    with torch.cuda.amp.autocast(enabled=config.mixed_precision):\n",
    "                        outputs = model(data, lengths)\n",
    "                        loss = criterion(outputs, targets)\n",
    "                        val_loss += loss.item()\n",
    "                        \n",
    "                except RuntimeError as e:\n",
    "                    logger.error(f\"Error in validation: {str(e)}\")\n",
    "                    raise\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        epoch_loss /= len(train_loader)\n",
    "        val_loss /= len(valid_loader)\n",
    "        \n",
    "        # Update training metrics\n",
    "        train_metrics['train_losses'].append(epoch_loss)\n",
    "        train_metrics['val_losses'].append(val_loss)\n",
    "        train_metrics['learning_rates'].append(current_lr)\n",
    "        \n",
    "        # Model checkpointing\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            train_metrics['best_epoch'] = epoch\n",
    "            early_stopping_counter = 0\n",
    "            \n",
    "            # Save checkpoint\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, 'batter_model.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'config': asdict(config),\n",
    "                'metrics': train_metrics,\n",
    "                'scaler_state_dict': scaler.state_dict()\n",
    "            }, checkpoint_path)\n",
    "            \n",
    "            logger.info(f'New best model saved with validation loss: {val_loss:.4f}')\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "        \n",
    "        # Log epoch metrics\n",
    "        logger.info(\n",
    "            f'Epoch {epoch+1}: '\n",
    "            f'Train Loss = {epoch_loss:.4f}, '\n",
    "            f'Val Loss = {val_loss:.4f}, '\n",
    "            f'LR = {current_lr:.2e}'\n",
    "        )\n",
    "        \n",
    "        # Early stopping check\n",
    "        if early_stopping_counter >= config.early_stopping_patience:\n",
    "            logger.info(f'Early stopping triggered after {epoch+1} epochs')\n",
    "            break\n",
    "    \n",
    "    return train_metrics\n",
    "\n",
    "# Initialize training\n",
    "try:\n",
    "    metrics = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        valid_loader=valid_loader,\n",
    "        config=config,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        criterion=criterion\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Training failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batter_names(raw_df: pd.DataFrame, output_path: str = '../data/batter_names.csv'):\n",
    "    \"\"\"Generate and save a dataset of batter names and IDs\"\"\"\n",
    "    try:\n",
    "        # Get unique batter entries\n",
    "        batter_names = raw_df[['Name', 'IDfg']].drop_duplicates()\n",
    "        \n",
    "        # Sort by Name for easier reference\n",
    "        batter_names = batter_names.sort_values('Name')\n",
    "        \n",
    "        # Save to CSV\n",
    "        batter_names.to_csv(output_path, index=False)\n",
    "        logger.info(f\"Saved {len(batter_names)} batter names to {output_path}\")\n",
    "        \n",
    "        return batter_names\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating batter names: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error generating batter names: name 'pitcher_names' is not defined\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pitcher_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 165\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# Load data and model\u001b[39;00m\n\u001b[0;32m    164\u001b[0m raw_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/mlb_batting_data_2010_2024.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 165\u001b[0m player_names \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_batter_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_df\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Replace loading CSV with function call\u001b[39;00m\n\u001b[0;32m    166\u001b[0m scaler \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatter_scaler.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# Initialize configuration with both existing and new rate stats\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m, in \u001b[0;36mgenerate_batter_names\u001b[1;34m(raw_df, output_path)\u001b[0m\n\u001b[0;32m      5\u001b[0m batter_names \u001b[38;5;241m=\u001b[39m raw_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIDfg\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mdrop_duplicates()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Sort by Name for easier reference\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m batter_names \u001b[38;5;241m=\u001b[39m \u001b[43mpitcher_names\u001b[49m\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Save to CSV\u001b[39;00m\n\u001b[0;32m     11\u001b[0m batter_names\u001b[38;5;241m.\u001b[39mto_csv(output_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pitcher_names' is not defined"
     ]
    }
   ],
   "source": [
    "def load_model_from_checkpoint(checkpoint_path: str, data_config, device: torch.device) -> nn.Module:\n",
    "    \"\"\"Load model with proper error handling and validation\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Loading model from {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        \n",
    "        # Initialize model with correct parameters\n",
    "        model = ImprovedLSTM(\n",
    "            input_size=len(data_config.input_features),\n",
    "            hidden_size=256,\n",
    "            num_layers=4,\n",
    "            output_size=len(data_config.input_features),\n",
    "            dropout=0.2,\n",
    "            bidirectional=True,\n",
    "            num_heads=8\n",
    "        ).to(device)\n",
    "        \n",
    "        # Load state dict\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        \n",
    "        # Validate model configuration\n",
    "        if hasattr(checkpoint, 'config'):\n",
    "            logger.info(f\"Loaded model config: {checkpoint['config']}\")\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Checkpoint file not found: {checkpoint_path}\")\n",
    "        raise\n",
    "    except KeyError as e:\n",
    "        logger.error(f\"Invalid checkpoint structure: {str(e)}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def predict_future_stats(player_id, input_features, model, scaler, raw_df, player_names):\n",
    "    \"\"\"Predict future stats for a given player with dynamic feature handling\"\"\"\n",
    "    # Get player data\n",
    "    player_data = raw_df[raw_df['IDfg'] == player_id].sort_values('Season')\n",
    "    if len(player_data) < 3:\n",
    "        logger.warning(f\"Insufficient data for player {player_id}\")\n",
    "        return None\n",
    "        \n",
    "    # Get player info\n",
    "    player_name = player_names[player_names['IDfg'] == player_id]['Name'].iloc[0]\n",
    "    last_season = player_data['Season'].max()\n",
    "    last_age = player_data[player_data['Season'] == last_season]['Age'].iloc[0]\n",
    "    \n",
    "    logger.info(f\"\\nGenerating predictions for {player_name}\")\n",
    "    logger.info(f\"Last season: {last_season}, Last age: {last_age}\")\n",
    "    \n",
    "    # Get device from model\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Get most recent sequence and calculate career stats\n",
    "    recent_data = player_data[input_features].iloc[-3:].copy()\n",
    "    career_stats = player_data[input_features].mean()\n",
    "    \n",
    "    # Create enhanced features with career stats\n",
    "    enhanced_features = []\n",
    "    for idx, row in recent_data.iterrows():\n",
    "        career_dev = row - career_stats\n",
    "        combined = np.concatenate([row.values, career_dev.values])\n",
    "        enhanced_features.append(combined)\n",
    "    \n",
    "    sequence = np.array(enhanced_features)\n",
    "    sequence_scaled = scaler.transform(sequence)\n",
    "    \n",
    "    # Dynamically get feature dimensions\n",
    "    n_features = len(input_features)\n",
    "    sequence_scaled = sequence_scaled[:, :n_features]\n",
    "    \n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for year_offset in range(1, 11):\n",
    "            current_year = last_season + year_offset\n",
    "            current_age = last_age + year_offset\n",
    "            \n",
    "            data = torch.FloatTensor(sequence_scaled).unsqueeze(0).to(device)\n",
    "            lengths = torch.tensor([3], dtype=torch.int64, device=device)\n",
    "            \n",
    "            output = model(data, lengths)\n",
    "            pred_numpy = output.cpu().numpy()[0]\n",
    "            \n",
    "            # Dynamic padding based on scaler dimensions\n",
    "            scaler_dim = scaler.n_features_in_\n",
    "            pred_padded = np.pad(pred_numpy, (0, scaler_dim - n_features), 'constant')\n",
    "            unscaled_pred = scaler.inverse_transform(pred_padded.reshape(1, -1))[0][:n_features]\n",
    "            \n",
    "            prediction = {\n",
    "                'Name': player_name,\n",
    "                'Age': current_age,\n",
    "                'Year': current_year,\n",
    "                'IDfg': player_id\n",
    "            }\n",
    "            \n",
    "            for i, feature in enumerate(input_features):\n",
    "                if feature == 'Age':\n",
    "                    prediction[feature] = current_age\n",
    "                else:\n",
    "                    prediction[feature] = unscaled_pred[i]\n",
    "            \n",
    "            predictions.append(prediction)\n",
    "            \n",
    "            # Update sequence for next prediction\n",
    "            age_index = input_features.index('Age')\n",
    "            # Create zero array of correct scaler dimension\n",
    "            age_update = np.zeros(scaler_dim)\n",
    "            age_update[age_index] = current_age\n",
    "            pred_numpy[age_index] = scaler.transform([age_update])[0][age_index]\n",
    "            sequence_scaled = np.vstack([sequence_scaled[1:], pred_numpy])\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def predict_all_2024_players(raw_df, player_names, model, scaler, input_features):\n",
    "    \"\"\"Predict future stats with proper feature dimension handling\"\"\"\n",
    "    logger.info(\"Starting predictions for all 2024 players\")\n",
    "    \n",
    "    # Get all players from 2024 with minimum PA\n",
    "    players_2024 = raw_df[\n",
    "        (raw_df['Season'] == 2024) & \n",
    "        (raw_df['PA'] >= 100)\n",
    "    ]['IDfg'].unique()\n",
    "    \n",
    "    logger.info(f\"Found {len(players_2024)} players from 2024\")\n",
    "    \n",
    "    all_predictions = []\n",
    "    \n",
    "    for player_id in tqdm(players_2024):\n",
    "        try:\n",
    "            predictions = predict_future_stats(\n",
    "                player_id=player_id,\n",
    "                input_features=input_features,\n",
    "                model=model,\n",
    "                scaler=scaler,\n",
    "                raw_df=raw_df,\n",
    "                player_names=player_names\n",
    "            )\n",
    "            if predictions:\n",
    "                all_predictions.extend(predictions)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error predicting for player {player_id}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Create DataFrame only if predictions exist\n",
    "    predictions_df = pd.DataFrame(all_predictions) if all_predictions else None\n",
    "    \n",
    "    if predictions_df is not None:\n",
    "        # Save separate CSV for each year\n",
    "        for year in range(2025, 2035):\n",
    "            year_predictions = predictions_df[predictions_df['Year'] == year].copy()\n",
    "            year_predictions = year_predictions.sort_values('wRC+', ascending=False)\n",
    "            filename = f'../data/generated/Batters_{year}_Predictions.csv'\n",
    "            year_predictions.to_csv(filename, index=False)\n",
    "            logger.info(f\"Saved predictions for {year} to {filename}\")\n",
    "        return predictions_df\n",
    "    else:\n",
    "        logger.warning(\"No predictions were generated\")\n",
    "        return None\n",
    "\n",
    "# Load data and model\n",
    "raw_df = pd.read_csv('../data/mlb_batting_data_2010_2024.csv')\n",
    "player_names = generate_batter_names(raw_df)  # Replace loading CSV with function call\n",
    "scaler = joblib.load('batter_scaler.pkl')\n",
    "\n",
    "# Initialize configuration with both existing and new rate stats\n",
    "data_config = DataConfig(\n",
    "    input_features = [\n",
    "        # Base features\n",
    "        'Age', 'BB%', 'K%', 'BABIP', 'AVG', 'OBP', 'SLG', 'wOBA', 'wRC+',\n",
    "        'Barrel%', 'HardHit%', 'EV',\n",
    "        'GB%', 'FB%', 'LD%', 'Pull%', 'Oppo%', 'O-Swing%', 'Z-Swing%',\n",
    "        'Swing%', 'O-Contact%', 'Z-Contact%',\n",
    "        'Contact%', 'SwStr%', 'CSW%', 'TTO%',\n",
    "        \n",
    "        # Only rate stats we can calculate\n",
    "        'wSB_rate', 'UBR_rate', 'wGDP_rate', 'Def_rate'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Modify the raw dataframe to include rate stats before loading model\n",
    "raw_df = calculate_rate_stats(raw_df)\n",
    "\n",
    "# Rest of the code remains the same\n",
    "model = load_model_from_checkpoint(\n",
    "    checkpoint_path='checkpoints/batter_model.pth',\n",
    "    data_config=data_config,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "predictions_df = predict_all_2024_players(\n",
    "    raw_df=raw_df,\n",
    "    player_names=player_names,  # Now using the generated player_names\n",
    "    model=model,\n",
    "    scaler=scaler,\n",
    "    input_features=data_config.input_features\n",
    ")\n",
    "\n",
    "if predictions_df is not None:\n",
    "    print(\"\\nTop 10 Predicted Performers for 2025:\")\n",
    "    print(predictions_df[predictions_df['Year'] == 2025][\n",
    "        ['Name', 'Age', 'wRC+', 'wSB_rate','Def_rate']  # Added rate stats to output\n",
    "    ].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Saved predictions with WAR components for 2025 to ../data/Batters_2025_Predictions.csv\n",
      "INFO:__main__:Saved predictions with WAR components for 2026 to ../data/Batters_2026_Predictions.csv\n",
      "INFO:__main__:Saved predictions with WAR components for 2027 to ../data/Batters_2027_Predictions.csv\n",
      "INFO:__main__:Saved predictions with WAR components for 2028 to ../data/Batters_2028_Predictions.csv\n",
      "INFO:__main__:Saved predictions with WAR components for 2029 to ../data/Batters_2029_Predictions.csv\n",
      "INFO:__main__:Saved predictions with WAR components for 2030 to ../data/Batters_2030_Predictions.csv\n",
      "INFO:__main__:Saved predictions with WAR components for 2031 to ../data/Batters_2031_Predictions.csv\n",
      "INFO:__main__:Saved predictions with WAR components for 2032 to ../data/Batters_2032_Predictions.csv\n",
      "INFO:__main__:Saved predictions with WAR components for 2033 to ../data/Batters_2033_Predictions.csv\n",
      "INFO:__main__:Saved predictions with WAR components for 2034 to ../data/Batters_2034_Predictions.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Predicted WAR for 2025:\n",
      "                   Name  Age      wOBA        wRC+       WAR        Off  \\\n",
      "0           Aaron Judge   33  0.425214  175.472733  7.157799  59.524133   \n",
      "490  Fernando Tatis Jr.   26  0.381567  145.896698  5.859888  37.168632   \n",
      "10       Bobby Witt Jr.   25  0.364445  134.782791  5.750863  28.398903   \n",
      "860        Austin Riley   28  0.387687  150.994949  5.509397  40.302852   \n",
      "230         Kyle Tucker   28  0.369967  139.134521  5.300707  31.227068   \n",
      "180        Corey Seager   31  0.373329  137.474228  5.270177  32.948809   \n",
      "40     Gunnar Henderson   24  0.366878  140.851181  5.150321  29.645044   \n",
      "250       Rafael Devers   28  0.387060  147.277390  5.120308  39.981807   \n",
      "120         Cal Raleigh   28  0.343027  119.611732  4.963040  17.428583   \n",
      "30            Juan Soto   26  0.374874  146.304413  4.655615  33.740522   \n",
      "\n",
      "          BsR        Def  \n",
      "0    0.014833  -8.293755  \n",
      "490  2.109650  -0.752593  \n",
      "10   1.658754   7.399585  \n",
      "860 -0.284647  -4.927330  \n",
      "230  1.662776   0.155864  \n",
      "180 -0.475050   0.272753  \n",
      "40   1.100406   0.826480  \n",
      "250 -0.612696  -8.091312  \n",
      "120  0.407175  11.900812  \n",
      "30  -0.454918  -6.561800  \n"
     ]
    }
   ],
   "source": [
    "def calculate_war_components(row, games=150):\n",
    "    \"\"\"Calculate WAR components from rate stats and convert to counting stats\"\"\"\n",
    "    # Constants\n",
    "    wOBA_scale = 1.23  # 2024 value\n",
    "    lg_wOBA = 0.309    # 2024 league average \n",
    "    RPW = 9.8          # Runs per win for 2024\n",
    "    PA_per_game = 4.2  # Average PA per game\n",
    "    lgPA = 186188      # 2024 league total PA\n",
    "    \n",
    "    # Calculate PA\n",
    "    pa = games * PA_per_game\n",
    "    \n",
    "    # 1. Batting Runs (Off)\n",
    "    Off = ((row['wOBA'] - lg_wOBA) / wOBA_scale) * pa\n",
    "    \n",
    "    # 2. Base Running Runs (BsR)\n",
    "    BsR = (row['wSB_rate'] + row['UBR_rate'] + row['wGDP_rate']) * pa\n",
    "    \n",
    "    # 3. Fielding Runs (Def)\n",
    "    Def = row['Def_rate'] * games\n",
    "        \n",
    "    # 4. Replacement Level\n",
    "    rep_level = 570 * RPW * pa / lgPA\n",
    "    \n",
    "    # Calculate total RAR (Runs Above Replacement)\n",
    "    rar = Off + BsR + Def + rep_level\n",
    "    \n",
    "    # Convert to WAR\n",
    "    war = rar / RPW\n",
    "    \n",
    "    return war, {\n",
    "        'Off': Off,\n",
    "        'BsR': BsR,\n",
    "        'Def': Def,\n",
    "        'WAR': war\n",
    "    }\n",
    "\n",
    "def calculate_war_for_predictions(predictions_df, games=150):\n",
    "    \"\"\"Add WAR and its components to predictions dataframe\"\"\"\n",
    "    # Initialize new columns\n",
    "    war_components = ['Off', 'BsR', 'Def', 'WAR']\n",
    "    for col in war_components:\n",
    "        predictions_df[col] = 0.0\n",
    "    \n",
    "    # Calculate WAR for each row\n",
    "    for idx, row in predictions_df.iterrows():\n",
    "        _, components = calculate_war_components(row, games)\n",
    "        for component, value in components.items():\n",
    "            predictions_df.at[idx, component] = value\n",
    "    \n",
    "    return predictions_df\n",
    "\n",
    "# After generating predictions, calculate WAR and save to CSVs\n",
    "if predictions_df is not None:\n",
    "    predictions_df = calculate_war_for_predictions(predictions_df)\n",
    "    \n",
    "    # Save predictions with WAR components to separate year CSVs\n",
    "    for year in range(2025, 2035):\n",
    "        # Load existing file\n",
    "        filename = f'../data/Batters_{year}_Predictions.csv'\n",
    "        year_predictions = predictions_df[predictions_df['Year'] == year].copy()\n",
    "        \n",
    "        # Sort by WAR\n",
    "        year_predictions = year_predictions.sort_values('WAR', ascending=False)\n",
    "        \n",
    "        # Select columns to save, including new WAR components\n",
    "        cols_to_save = [col for col in year_predictions.columns if col not in ['RAR', 'rep_level']]\n",
    "        \n",
    "        # Save updated predictions\n",
    "        year_predictions[cols_to_save].to_csv(filename, index=False)\n",
    "        logger.info(f\"Saved predictions with WAR components for {year} to {filename}\")\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nTop 10 Predicted WAR for 2025:\")\n",
    "    display_cols = ['Name', 'Age', 'wOBA', 'wRC+', 'WAR', 'Off', 'BsR', 'Def']\n",
    "    print(predictions_df[predictions_df['Year'] == 2025][display_cols]\n",
    "          .sort_values('WAR', ascending=False).head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
