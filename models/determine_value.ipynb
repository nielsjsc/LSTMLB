{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-02 17:06:52,409 - INFO - Initialized MLB Trade Simulator value determination module\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MLB Trade Simulator - Value Determination Module\n",
    "Author: Niels Christoffersen\n",
    "Version: 1.0\n",
    "Last Updated: 12/23/2024\n",
    "\n",
    "This module calculates player values based on WAR projections and contract status.\n",
    "It handles data loading, cleaning, and value calculations for MLB players.\n",
    "\"\"\"\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Type aliases\n",
    "PathLike = str | Path\n",
    "\n",
    "# Global constants\n",
    "ROOT_DIR = Path(os.getcwd()).parent  # Changed from Path(__file__).parent.parent\n",
    "DATA_DIR = ROOT_DIR / 'data'\n",
    "GENERATED_DIR = DATA_DIR / 'generated'\n",
    "OUTPUT_DIR = GENERATED_DIR / 'value_by_year'\n",
    "HITTER_COLUMNS = [\n",
    "    'Name', 'Age', 'IDfg', 'BB%', 'K%', 'AVG', 'OBP', 'SLG', 'wOBA', \n",
    "    'wRC+', 'EV', 'Off', 'BsR', 'Def', 'WAR'\n",
    "]\n",
    "\n",
    "PITCHER_COLUMNS = [\n",
    "    'Name', 'Age', 'IDfg', 'FIP', 'SIERA', 'K%', 'BB%', 'GB%', 'FB%',\n",
    "    'Stuff+', 'Location+', 'Pitching+', 'FBv', 'WAR'\n",
    "]\n",
    "# Ensure required directories exist\n",
    "for directory in [DATA_DIR, GENERATED_DIR, OUTPUT_DIR]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "    logger.debug(f\"Verified directory exists: {directory}\")\n",
    "\n",
    "logger.info(\"Initialized MLB Trade Simulator value determination module\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-02 17:06:52,447 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\SP_Predictions_2025.csv\n",
      "2025-01-02 17:06:52,458 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\SP_Predictions_2026.csv\n",
      "2025-01-02 17:06:52,464 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\SP_Predictions_2027.csv\n",
      "2025-01-02 17:06:52,470 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\SP_Predictions_2028.csv\n",
      "2025-01-02 17:06:52,476 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\SP_Predictions_2029.csv\n",
      "2025-01-02 17:06:52,483 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\SP_Predictions_2030.csv\n",
      "2025-01-02 17:06:52,490 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\SP_Predictions_2031.csv\n",
      "2025-01-02 17:06:52,498 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\SP_Predictions_2032.csv\n",
      "2025-01-02 17:06:52,506 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\SP_Predictions_2033.csv\n",
      "2025-01-02 17:06:52,514 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\SP_Predictions_2034.csv\n",
      "2025-01-02 17:06:52,520 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\SP_Predictions_2035.csv\n",
      "2025-01-02 17:06:52,527 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\SP_Predictions_2036.csv\n",
      "2025-01-02 17:06:52,533 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\SP_Predictions_2037.csv\n",
      "2025-01-02 17:06:52,540 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\SP_Predictions_2038.csv\n",
      "2025-01-02 17:06:52,548 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\SP_Predictions_2039.csv\n",
      "2025-01-02 17:06:52,558 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\RP_Predictions_2025.csv\n",
      "2025-01-02 17:06:52,567 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\RP_Predictions_2026.csv\n",
      "2025-01-02 17:06:52,577 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\RP_Predictions_2027.csv\n",
      "2025-01-02 17:06:52,586 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\RP_Predictions_2028.csv\n",
      "2025-01-02 17:06:52,594 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\RP_Predictions_2029.csv\n",
      "2025-01-02 17:06:52,603 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\RP_Predictions_2030.csv\n",
      "2025-01-02 17:06:52,611 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\RP_Predictions_2031.csv\n",
      "2025-01-02 17:06:52,619 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\RP_Predictions_2032.csv\n",
      "2025-01-02 17:06:52,628 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\RP_Predictions_2033.csv\n",
      "2025-01-02 17:06:52,636 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\RP_Predictions_2034.csv\n",
      "2025-01-02 17:06:52,644 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\RP_Predictions_2035.csv\n",
      "2025-01-02 17:06:52,653 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\RP_Predictions_2036.csv\n",
      "2025-01-02 17:06:52,662 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\RP_Predictions_2037.csv\n",
      "2025-01-02 17:06:52,669 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\RP_Predictions_2038.csv\n",
      "2025-01-02 17:06:52,678 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\RP_Predictions_2039.csv\n",
      "2025-01-02 17:06:52,691 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\Batter_Predictions_2025.csv\n",
      "2025-01-02 17:06:52,700 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\Batter_Predictions_2026.csv\n",
      "2025-01-02 17:06:52,706 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\Batter_Predictions_2027.csv\n",
      "2025-01-02 17:06:52,714 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\Batter_Predictions_2028.csv\n",
      "2025-01-02 17:06:52,721 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\Batter_Predictions_2029.csv\n",
      "2025-01-02 17:06:52,730 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\Batter_Predictions_2030.csv\n",
      "2025-01-02 17:06:52,736 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\Batter_Predictions_2031.csv\n",
      "2025-01-02 17:06:52,743 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\Batter_Predictions_2032.csv\n",
      "2025-01-02 17:06:52,750 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\Batter_Predictions_2033.csv\n",
      "2025-01-02 17:06:52,757 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\Batter_Predictions_2034.csv\n",
      "2025-01-02 17:06:52,765 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\Batter_Predictions_2035.csv\n",
      "2025-01-02 17:06:52,771 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\Batter_Predictions_2036.csv\n",
      "2025-01-02 17:06:52,779 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\Batter_Predictions_2037.csv\n",
      "2025-01-02 17:06:52,785 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\Batter_Predictions_2038.csv\n",
      "2025-01-02 17:06:52,792 - INFO - Loading c:\\Users\\User\\Desktop\\MLBTradeSim\\data\\generated\\Batter_Predictions_2039.csv\n",
      "2025-01-02 17:06:52,807 - INFO - Loaded 3330 SP, 6240 RP, 6090 batter, and 3821 salary records\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "PREDICTION_YEARS = range(2025, 2040)\n",
    "REQUIRED_COLUMNS = {\n",
    "    'predictions': ['Name', 'IDfg', 'Age', 'WAR'],\n",
    "    'salary': ['Name', 'IDfg', 'Salary', 'Contract_Status']\n",
    "}\n",
    "\n",
    "def validate_files_exist(pattern: str, years: range) -> None:\n",
    "    \"\"\"Validate prediction files exist for given years.\"\"\"\n",
    "    missing_files = [\n",
    "        f\"{pattern}_{year}.csv\" \n",
    "        for year in years \n",
    "        if not (GENERATED_DIR / f\"{pattern}_{year}.csv\").exists()\n",
    "    ]\n",
    "    if missing_files:\n",
    "        raise FileNotFoundError(f\"Missing files: {', '.join(missing_files)}\")\n",
    "\n",
    "def load_prediction_files(pattern: str, years: range = PREDICTION_YEARS) -> DataFrame:\n",
    "    \"\"\"Load and combine prediction CSVs with validation.\"\"\"\n",
    "    validate_files_exist(pattern, years)\n",
    "    dfs = []\n",
    "    \n",
    "    for year in years:\n",
    "        file_path = GENERATED_DIR / f\"{pattern}_{year}.csv\"\n",
    "        logger.info(f\"Loading {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            missing_cols = set(REQUIRED_COLUMNS['predictions']) - set(df.columns)\n",
    "            if missing_cols:\n",
    "                raise ValueError(f\"Missing columns in {file_path}: {missing_cols}\")\n",
    "                \n",
    "            df['prediction_year'] = year\n",
    "            dfs.append(df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading {file_path}: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Main data loading\n",
    "try:\n",
    "    sp_data = load_prediction_files('SP_Predictions')\n",
    "    rp_data = load_prediction_files('RP_Predictions')\n",
    "    batter_data = load_prediction_files('Batter_Predictions')\n",
    "    salary_data = pd.read_csv(DATA_DIR / 'SPORTRAC_MLB_SALARY_DATA.csv')\n",
    "    \n",
    "    logger.info(f\"Loaded {len(sp_data)} SP, {len(rp_data)} RP, \"\n",
    "                f\"{len(batter_data)} batter, and {len(salary_data)} salary records\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Critical error during data loading: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group Positions, and merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-02 17:06:52,852 - INFO - \n",
      "Prediction Data Summary:\n",
      "2025-01-02 17:06:52,884 - WARNING - \n",
      "Missing values found:\n",
      "WAR    15\n",
      "dtype: int64\n",
      "2025-01-02 17:06:52,885 - INFO - Successfully merged 15660 player predictions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                count   mean    std    min    max\n",
      "position_group prediction_year                                   \n",
      "POS            2025               405  0.987  1.647 -3.557  6.374\n",
      "               2026               405  0.586  1.876 -4.631  6.991\n",
      "               2027               405  0.143  2.045 -5.906  7.254\n",
      "               2028               405 -0.283  2.181 -6.005  7.274\n",
      "               2029               405 -0.722  2.288 -6.295  7.260\n",
      "               2030               405 -1.153  2.341 -6.335  7.099\n",
      "               2031               405 -1.565  2.342 -6.205  6.852\n",
      "               2032               405 -1.961  2.283 -6.328  6.477\n",
      "               2033               405 -2.335  2.160 -6.303  5.955\n",
      "               2034               405 -2.679  1.975 -6.303  5.289\n",
      "               2035               405 -2.978  1.752 -6.346  4.499\n",
      "               2036               405 -3.220  1.510 -6.404  3.734\n",
      "               2037               405 -3.400  1.266 -6.254  2.606\n",
      "               2038               405 -3.514  1.047 -6.100  1.328\n",
      "               2039               405 -3.566  0.872 -5.872  0.439\n",
      "RP             2025               416  0.706  0.410  0.100  2.600\n",
      "               2026               416  0.591  0.373  0.000  2.500\n",
      "               2027               416  0.523  0.352  0.000  2.400\n",
      "               2028               416  0.437  0.325  0.000  2.100\n",
      "               2029               416  0.394  0.289  0.000  1.900\n",
      "               2030               416  0.363  0.247  0.000  1.600\n",
      "               2031               416  0.344  0.215  0.000  1.300\n",
      "               2032               416  0.330  0.190  0.100  1.100\n",
      "               2033               416  0.330  0.177  0.100  1.000\n",
      "               2034               416  0.331  0.169  0.100  0.900\n",
      "               2035               416  0.341  0.167  0.100  1.000\n",
      "               2036               416  0.348  0.165  0.100  1.000\n",
      "               2037               416  0.358  0.168  0.100  1.000\n",
      "               2038               416  0.361  0.160  0.100  1.000\n",
      "               2039               416  0.364  0.156  0.100  0.900\n",
      "SP             2025               222  1.905  1.300 -0.500  5.300\n",
      "               2026               222  1.887  1.488 -0.500  4.900\n",
      "               2027               222  1.533  1.559 -0.700  4.700\n",
      "               2028               222  1.133  1.519 -0.900  4.400\n",
      "               2029               222  0.836  1.460 -0.900  4.000\n",
      "               2030               222  0.597  1.409 -1.000  3.800\n",
      "               2031               222  0.388  1.340 -1.000  3.600\n",
      "               2032               222  0.193  1.249 -1.000  3.500\n",
      "               2033               222  0.008  1.154 -1.000  3.400\n",
      "               2034               222 -0.149  1.065 -1.000  3.300\n",
      "               2035               222 -0.282  0.980 -1.000  3.300\n",
      "               2036               222 -0.402  0.894 -1.000  3.200\n",
      "               2037               222 -0.497  0.795 -1.000  3.100\n",
      "               2038               222 -0.586  0.681 -1.000  2.900\n",
      "               2039               222 -0.669  0.566 -1.000  2.700\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Position Grouping and Data Merging\n",
    "- Groups player positions into SP/RP/POS categories\n",
    "- Merges prediction datasets\n",
    "- Validates data quality\n",
    "- Provides summary statistics\n",
    "\"\"\"\n",
    "\n",
    "# Add position grouping\n",
    "sp_data['position_group'] = 'SP'\n",
    "rp_data['position_group'] = 'RP'\n",
    "batter_data['position_group'] = 'POS'\n",
    "\n",
    "def merge_prediction_data(sp_df, rp_df, batter_df):\n",
    "    \"\"\"Merge prediction datasets with validation.\"\"\"\n",
    "    required_cols = ['Name', 'IDfg', 'position_group', 'Age', 'prediction_year', 'WAR']\n",
    "    \n",
    "    # Validate columns exist\n",
    "    for df, name in [(sp_df, 'SP'), (rp_df, 'RP'), (batter_df, 'Batter')]:\n",
    "        missing = set(required_cols) - set(df.columns)\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing columns in {name} data: {missing}\")\n",
    "\n",
    "    # Combine datasets\n",
    "    player_predictions = pd.concat([\n",
    "        sp_df[required_cols],\n",
    "        rp_df[required_cols],\n",
    "        batter_df[required_cols]\n",
    "    ], ignore_index=True)\n",
    "    \n",
    "    return player_predictions\n",
    "\n",
    "# Merge data and generate summary\n",
    "try:\n",
    "    player_predictions = merge_prediction_data(sp_data, rp_data, batter_data)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    summary = player_predictions.groupby(['position_group', 'prediction_year'])['WAR'].agg([\n",
    "        'count',\n",
    "        'mean',\n",
    "        'std',\n",
    "        'min',\n",
    "        'max'\n",
    "    ]).round(3)\n",
    "    \n",
    "    logger.info(\"\\nPrediction Data Summary:\")\n",
    "    print(summary)\n",
    "    \n",
    "    # Validate no missing values\n",
    "    missing_values = player_predictions.isnull().sum()\n",
    "    if missing_values.any():\n",
    "        logger.warning(f\"\\nMissing values found:\\n{missing_values[missing_values > 0]}\")\n",
    "        \n",
    "    logger.info(f\"Successfully merged {len(player_predictions)} player predictions\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error merging prediction data: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-02 17:06:52,917 - INFO - Starting salary data cleaning process\n",
      "2025-01-02 17:06:52,944 - INFO - \n",
      "Status distribution:\n",
      "2025-01-02 17:06:52,945 - INFO - Status\n",
      "Estimate       871\n",
      "PRE-ARB        740\n",
      "UFA            423\n",
      "ARB 1          278\n",
      "ARB 3          213\n",
      "              ... \n",
      "$2,370,968       1\n",
      "$36,571,428      1\n",
      "$28,071,428      1\n",
      "$10,015,872      1\n",
      "$2,962,963       1\n",
      "Name: count, Length: 134, dtype: int64\n",
      "2025-01-02 17:06:52,947 - INFO - \n",
      "Salary cleaning summary:\n",
      "2025-01-02 17:06:52,948 - INFO - original_rows: 3821\n",
      "2025-01-02 17:06:52,949 - INFO - cleaned_rows: 3806\n",
      "2025-01-02 17:06:52,949 - INFO - valid_salary_rows: 2043\n",
      "2025-01-02 17:06:52,950 - INFO - min_salary: 250,000.00\n",
      "2025-01-02 17:06:52,951 - INFO - max_salary: 51,875,000.00\n",
      "2025-01-02 17:06:52,951 - INFO - mean_salary: 10,248,524.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of cleaned salary data:\n",
      "      Player Name    Year Team     Payroll   Status\n",
      "0  Corbin Carroll  2023.0  ari   1625000.0      NaN\n",
      "1  Corbin Carroll  2024.0  ari   3625000.0  Pre-Arb\n",
      "2  Corbin Carroll  2025.0  ari   5625000.0  Pre-Arb\n",
      "3  Corbin Carroll  2026.0  ari  10625000.0    ARB 1\n",
      "4  Corbin Carroll  2027.0  ari  12625000.0    ARB 2\n",
      "\n",
      "Data validation:\n",
      "Null values:\n",
      "Player Name       0\n",
      "Year              0\n",
      "Team              0\n",
      "Payroll        1763\n",
      "Status          407\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Salary Data Processing Module\n",
    "- Cleans and standardizes salary data\n",
    "- Preserves contract status information\n",
    "- Handles missing and invalid values\n",
    "- Validates data quality\n",
    "\"\"\"\n",
    "\n",
    "def clean_salary_data(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Clean and standardize salary data from Sportrac.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Raw salary data with Payroll and Status columns\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Cleaned salary data with standardized values\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting salary data cleaning process\")\n",
    "    \n",
    "    cleaned_df = df.copy()\n",
    "    \n",
    "    try:\n",
    "        # Remove non-player rows (options, buyouts, etc)\n",
    "        cleaned_df = cleaned_df[~cleaned_df['Player Name'].str.contains(\n",
    "            'OPT-OUT|UFA|PLAYER OPT|CLUB OPT', \n",
    "            na=False, \n",
    "            case=False\n",
    "        )]\n",
    "        \n",
    "        # Clean Year column\n",
    "        cleaned_df['Year'] = pd.to_numeric(cleaned_df['Year'], errors='coerce')\n",
    "        cleaned_df = cleaned_df.dropna(subset=['Year'])\n",
    "        \n",
    "        # Clean Payroll column - two-step process\n",
    "        payroll = (cleaned_df['Payroll']\n",
    "                  .astype(str)\n",
    "                  .str.replace('$', '', regex=False)\n",
    "                  .str.replace(',', '', regex=False)\n",
    "                  .str.replace('-', '', regex=False))\n",
    "        \n",
    "        cleaned_df['Payroll'] = pd.to_numeric(payroll, errors='coerce')\n",
    "        \n",
    "        # Status validation and cleaning\n",
    "        if 'Status' not in cleaned_df.columns:\n",
    "            logger.warning(\"Status column missing from input data\")\n",
    "        else:\n",
    "            status_counts = cleaned_df['Status'].value_counts()\n",
    "            logger.info(\"\\nStatus distribution:\")\n",
    "            logger.info(status_counts)\n",
    "        \n",
    "        # Generate summary statistics\n",
    "        stats = {\n",
    "            'original_rows': len(df),\n",
    "            'cleaned_rows': len(cleaned_df),\n",
    "            'valid_salary_rows': cleaned_df['Payroll'].notna().sum(),\n",
    "            'min_salary': cleaned_df['Payroll'].min(),\n",
    "            'max_salary': cleaned_df['Payroll'].max(),\n",
    "            'mean_salary': cleaned_df['Payroll'].mean()\n",
    "        }\n",
    "        \n",
    "        logger.info(\"\\nSalary cleaning summary:\")\n",
    "        for key, value in stats.items():\n",
    "            logger.info(f\"{key}: {value:,.2f}\" if isinstance(value, float) else f\"{key}: {value}\")\n",
    "            \n",
    "        return cleaned_df[['Player Name', 'Year', 'Team', 'Payroll', 'Status']].copy()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error cleaning salary data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Execute cleaning\n",
    "try:\n",
    "    salary_data_clean = clean_salary_data(salary_data)\n",
    "    \n",
    "    print(\"\\nSample of cleaned salary data:\")\n",
    "    print(salary_data_clean.head())\n",
    "    \n",
    "    print(\"\\nData validation:\")\n",
    "    print(f\"Null values:\\n{salary_data_clean.isnull().sum()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to process salary data: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-02 17:06:53,056 - INFO - \n",
      "Merge Results:\n",
      "2025-01-02 17:06:53,057 - INFO - Total players: 1137\n",
      "2025-01-02 17:06:53,057 - INFO - Matched: 695\n",
      "2025-01-02 17:06:53,058 - INFO - Unmatched: 442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample unmatched players:\n",
      "['Tim Tawa' 'Jordan Lawlar' 'Joe Elbis' 'Cristian Mena' 'Jorge Barrosa'\n",
      " 'Yilber Diaz' 'Adrian Del Castillo' 'Blake Walston' 'Slade Cecconi'\n",
      " 'Blaze Alexander']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Player Reference and ID Integration with Enhanced Name Matching\n",
    "Handles UTF-8 encoding and accent normalization\n",
    "\"\"\"\n",
    "\n",
    "import unidecode\n",
    "from thefuzz import fuzz\n",
    "\n",
    "def normalize_name(name: str) -> str:\n",
    "    \"\"\"Normalize player names by removing accents and standardizing format.\"\"\"\n",
    "    if pd.isna(name):\n",
    "        return name\n",
    "    return unidecode.unidecode(str(name)).upper().strip()\n",
    "\n",
    "def create_player_reference(sp_df: pd.DataFrame, \n",
    "                          rp_df: pd.DataFrame, \n",
    "                          batter_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create unified player reference with normalized names.\"\"\"\n",
    "    player_ref = pd.concat([\n",
    "        sp_df[['Name', 'IDfg', 'position_group']],\n",
    "        rp_df[['Name', 'IDfg', 'position_group']],\n",
    "        batter_df[['Name', 'IDfg', 'position_group']]\n",
    "    ]).drop_duplicates()\n",
    "    \n",
    "    player_ref['Name_Normalized'] = player_ref['Name'].apply(normalize_name)\n",
    "    return player_ref\n",
    "\n",
    "def merge_salary_with_ids(salary_df: pd.DataFrame, \n",
    "                         player_ref: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Merge salary data with player reference using normalized names.\"\"\"\n",
    "    # Normalize salary data names\n",
    "    salary_df['Name_Normalized'] = salary_df['Player Name'].apply(normalize_name)\n",
    "    \n",
    "    # Perform merge\n",
    "    merged_df = salary_df.merge(\n",
    "        player_ref[['Name_Normalized', 'IDfg']],\n",
    "        on='Name_Normalized',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Log matching statistics\n",
    "    total = len(salary_df['Player Name'].unique())\n",
    "    matched = len(merged_df[merged_df['IDfg'].notna()]['Player Name'].unique())\n",
    "    \n",
    "    logger.info(f\"\\nMerge Results:\")\n",
    "    logger.info(f\"Total players: {total}\")\n",
    "    logger.info(f\"Matched: {matched}\")\n",
    "    logger.info(f\"Unmatched: {total - matched}\")\n",
    "    \n",
    "    return merged_df.drop('Name_Normalized', axis=1)\n",
    "\n",
    "try:\n",
    "    player_ref = create_player_reference(sp_data, rp_data, batter_data)\n",
    "    salary_data_with_id = merge_salary_with_ids(salary_data_clean, player_ref)\n",
    "    \n",
    "    # Display unmatched players\n",
    "    unmatched = salary_data_with_id[salary_data_with_id['IDfg'].isna()]\n",
    "    if not unmatched.empty:\n",
    "        print(\"\\nSample unmatched players:\")\n",
    "        print(unmatched['Player Name'].unique()[:10])\n",
    "        \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in ID integration: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-02 17:06:53,095 - INFO - Processing contract statuses\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contract Processing Summary:\n",
      "Total players: 699\n",
      "Players with FA years: 695\n",
      "\n",
      "FA Years by Status:\n",
      "                      count     mean\n",
      "Status                              \n",
      "$1,250,000                1  2027.00\n",
      "$1,750,000                1  2026.00\n",
      "$1,800,000                1  2026.00\n",
      "$1,950,000                0      NaN\n",
      "$10,000,000               5  2026.80\n",
      "...                     ...      ...\n",
      "Pre-Arb                  33  2030.30\n",
      "RFA / QO                  0      NaN\n",
      "UFA                     384  2027.95\n",
      "Vesting                   9  2030.11\n",
      "arbitration-bypassed      2  2028.50\n",
      "\n",
      "[134 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Contract Status Processing Module\n",
    "- Determines Free Agency years for all players\n",
    "- Handles arbitration progression\n",
    "- Processes contract options and UFA designations\n",
    "- Validates contract timelines\n",
    "\"\"\"\n",
    "\n",
    "def determine_fa_year(status: str, current_year: int) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Calculate Free Agency year based on current status.\n",
    "    \n",
    "    Args:\n",
    "        status: Player's current contract status\n",
    "        current_year: Current season year\n",
    "        \n",
    "    Returns:\n",
    "        Optional[int]: Year player reaches free agency\n",
    "    \"\"\"\n",
    "    if not status or pd.isna(status):\n",
    "        return None\n",
    "        \n",
    "    status = str(status).upper().strip()\n",
    "    \n",
    "    # Direct FA indicators\n",
    "    if any(x in status for x in ['UFA', 'OPT-OUT', 'PLAYER']):\n",
    "        return current_year\n",
    "    \n",
    "    # Service time progression\n",
    "    status_to_years = {\n",
    "        'ESTIMATE': 6,\n",
    "        'PRE-ARB': 4,\n",
    "        'ARB1': 3,\n",
    "        'ARB 1': 3,\n",
    "        'ARB2': 2,\n",
    "        'ARB 2': 2,\n",
    "        'ARB3': 1,\n",
    "        'ARB 3': 1,\n",
    "        'ARB4': 1,\n",
    "        'ARB 4': 1\n",
    "    }\n",
    "    \n",
    "    for key, years in status_to_years.items():\n",
    "        if key in status:\n",
    "            return current_year + years\n",
    "            \n",
    "    return None\n",
    "\n",
    "\"\"\"\n",
    "Contract Status Processing with IDfg\n",
    "Determines FA years based on latest available status\n",
    "\"\"\"\n",
    "\n",
    "def process_contract_statuses(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Process latest contract status for each player.\"\"\"\n",
    "    logger.info(\"Processing contract statuses\")\n",
    "    \n",
    "    # Get latest year for each player\n",
    "    latest_status = (df[df['IDfg'].notna()]\n",
    "                    .sort_values('Year', ascending=True)\n",
    "                    .groupby('IDfg')\n",
    "                    .last()\n",
    "                    .reset_index())\n",
    "    \n",
    "    # Calculate FA years\n",
    "    fa_years = {}\n",
    "    for _, player in latest_status.iterrows():\n",
    "        status = str(player.get('Status', '')).upper().strip()\n",
    "        current_year = int(player['Year'])\n",
    "        \n",
    "        # Direct FA indicators\n",
    "        if any(x in status for x in ['UFA', 'OPT-OUT', 'PLAYER']):\n",
    "            fa_years[player['IDfg']] = current_year\n",
    "        # Next year FA\n",
    "        elif any(x in status for x in ['CLUB', 'VESTING', 'ARB3', 'ARB 3', 'ARB4', 'ARB 4']):\n",
    "            fa_years[player['IDfg']] = current_year + 1\n",
    "        # Service time progression\n",
    "        elif any(x in status for x in ['ARB2', 'ARB 2']):\n",
    "            fa_years[player['IDfg']] = current_year + 2\n",
    "        elif any(x in status for x in ['ARB1', 'ARB 1']):\n",
    "            fa_years[player['IDfg']] = current_year + 3\n",
    "        elif 'PRE' in status and 'ARB' in status:\n",
    "            fa_years[player['IDfg']] = current_year + 4\n",
    "        elif status == 'ESTIMATE':\n",
    "            fa_years[player['IDfg']] = current_year + 6\n",
    "    \n",
    "    # Map FA years back to original data\n",
    "    df['FA_Year'] = df['IDfg'].map(fa_years)\n",
    "    \n",
    "    return df\n",
    "\n",
    "try:\n",
    "    contract_data = process_contract_statuses(salary_data_with_id)\n",
    "    \n",
    "    # Output validation summary\n",
    "    print(\"\\nContract Processing Summary:\")\n",
    "    print(f\"Total players: {len(contract_data['IDfg'].unique())}\")\n",
    "    print(f\"Players with FA years: {len(contract_data[contract_data['FA_Year'].notna()]['IDfg'].unique())}\")\n",
    "    print(\"\\nFA Years by Status:\")\n",
    "    print(contract_data.groupby('Status')['FA_Year'].agg(['count', 'mean']).round(2))\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to process contract statuses: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-02 17:06:53,184 - INFO - Generating contract timeline\n",
      "2025-01-02 17:06:54,937 - INFO - Generated 3387 timeline records\n"
     ]
    }
   ],
   "source": [
    "def generate_contract_timeline(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Generate timeline until each player's FA year.\"\"\"\n",
    "    logger.info(\"Generating contract timeline\")\n",
    "    \n",
    "    # Handle duplicates\n",
    "    df_unique = (df.sort_values('Payroll', ascending=False)\n",
    "                  .drop_duplicates(subset=['IDfg', 'Year'], keep='first'))\n",
    "    \n",
    "    # Create lookup dict\n",
    "    base_data = {idfg: df[df['IDfg'] == idfg].iloc[0].to_dict() \n",
    "                 for idfg in df['IDfg'].unique() if not pd.isna(idfg)}\n",
    "    \n",
    "    all_rows = []\n",
    "    years = range(2025, 2040)\n",
    "    \n",
    "    for idfg, base_row in base_data.items():\n",
    "        fa_year = base_row.get('FA_Year')\n",
    "        if pd.isna(fa_year):\n",
    "            continue\n",
    "            \n",
    "        for year in years:\n",
    "            # Stop at FA year\n",
    "            if year > fa_year:\n",
    "                continue\n",
    "                \n",
    "            # Check existing data\n",
    "            existing = df_unique[\n",
    "                (df_unique['IDfg'] == idfg) & \n",
    "                (df_unique['Year'] == year)\n",
    "            ]\n",
    "            \n",
    "            if not existing.empty:\n",
    "                all_rows.append(existing.iloc[0].to_dict())\n",
    "                continue\n",
    "                \n",
    "            # Generate new row\n",
    "            new_row = base_row.copy()\n",
    "            new_row['Year'] = year\n",
    "            \n",
    "            # Calculate status\n",
    "            years_to_fa = fa_year - year\n",
    "            if years_to_fa <= 0:\n",
    "                new_row['Status'] = 'FA'\n",
    "            elif years_to_fa <= 1:\n",
    "                new_row['Status'] = 'ARB3'\n",
    "            elif years_to_fa <= 2:\n",
    "                new_row['Status'] = 'ARB2'\n",
    "            elif years_to_fa <= 3:\n",
    "                new_row['Status'] = 'ARB1'\n",
    "            else:\n",
    "                new_row['Status'] = 'Pre-ARB'\n",
    "                \n",
    "            new_row['Payroll'] = None\n",
    "            all_rows.append(new_row)\n",
    "    \n",
    "    result = pd.DataFrame(all_rows)\n",
    "    return result[df.columns].sort_values(['IDfg', 'Year'])\n",
    "\n",
    "# Execute timeline generation\n",
    "try:\n",
    "    contract_timeline = generate_contract_timeline(contract_data)\n",
    "    logger.info(f\"Generated {len(contract_timeline)} timeline records\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Timeline generation failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-02 17:06:55,016 - INFO - Processed 3387 rows\n",
      "2025-01-02 17:06:55,018 - INFO - Average WAR value: $8,745,984.29\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "WAR Value Calculation Module\n",
    "Applies tiered WAR values and inflation adjustments\n",
    "\"\"\"\n",
    "\n",
    "# Constants\n",
    "WAR_VALUE_TIERS = {\n",
    "    'tier1': {'max': 2, 'value': 6_000_000},\n",
    "    'tier2': {'max': 4, 'value': 9_000_000},\n",
    "    'tier3': {'value': 12_000_000}\n",
    "}\n",
    "INFLATION_RATE = 0.05\n",
    "BASE_YEAR = 2025\n",
    "\n",
    "def calculate_inflation_multiplier(year: int) -> float:\n",
    "    \"\"\"Calculate inflation multiplier from base year.\"\"\"\n",
    "    return (1 + INFLATION_RATE) ** (year - BASE_YEAR)\n",
    "\n",
    "def calculate_war_value(war: float, year: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate WAR value using tiered system and inflation.\n",
    "    \n",
    "    Args:\n",
    "        war (float): WAR value\n",
    "        year (int): Year for inflation adjustment\n",
    "    \"\"\"\n",
    "    if pd.isna(war) or war <= 0:\n",
    "        return 0.0\n",
    "        \n",
    "    value = 0.0\n",
    "    remaining_war = war\n",
    "    \n",
    "    # Tier 1: 0-2 WAR\n",
    "    tier1_war = min(remaining_war, WAR_VALUE_TIERS['tier1']['max'])\n",
    "    value += tier1_war * WAR_VALUE_TIERS['tier1']['value']\n",
    "    remaining_war -= tier1_war\n",
    "    \n",
    "    if remaining_war <= 0:\n",
    "        return value * calculate_inflation_multiplier(year)\n",
    "        \n",
    "    # Tier 2: 2-4 WAR\n",
    "    tier2_war = min(remaining_war, WAR_VALUE_TIERS['tier2']['max'] - WAR_VALUE_TIERS['tier1']['max'])\n",
    "    value += tier2_war * WAR_VALUE_TIERS['tier2']['value']\n",
    "    remaining_war -= tier2_war\n",
    "    \n",
    "    if remaining_war <= 0:\n",
    "        return value * calculate_inflation_multiplier(year)\n",
    "        \n",
    "    # Tier 3: 4+ WAR\n",
    "    value += remaining_war * WAR_VALUE_TIERS['tier3']['value']\n",
    "    \n",
    "    return value * calculate_inflation_multiplier(year)\n",
    "\n",
    "try:\n",
    "    # Join predictions with timeline\n",
    "    timeline_with_war = contract_timeline.merge(\n",
    "        player_predictions[['IDfg', 'prediction_year', 'WAR']],\n",
    "        left_on=['IDfg', 'Year'],\n",
    "        right_on=['IDfg', 'prediction_year'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Calculate WAR values\n",
    "    timeline_with_war['Base_Value'] = timeline_with_war.apply(\n",
    "        lambda x: calculate_war_value(x['WAR'], x['Year']), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Clean up and validate\n",
    "    timeline_with_war = timeline_with_war.drop('prediction_year', axis=1)\n",
    "    \n",
    "    logger.info(f\"Processed {len(timeline_with_war)} rows\")\n",
    "    logger.info(f\"Average WAR value: ${timeline_with_war['Base_Value'].mean():,.2f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to calculate WAR values: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-02 17:06:55,141 - INFO - \n",
      "Contract Value Calculation Summary:\n",
      "2025-01-02 17:06:55,142 - INFO - Total rows processed: 3387\n",
      "2025-01-02 17:06:55,143 - INFO - Rows with contract values: 2687\n",
      "2025-01-02 17:06:55,144 - INFO - \n",
      "Contract values by status:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      count          mean           std         min  \\\n",
      "Status                                                                \n",
      "$1,250,000              1.0  1.250000e+06           NaN   1250000.0   \n",
      "$1,750,000              1.0  1.750000e+06           NaN   1750000.0   \n",
      "$1,800,000              1.0  1.800000e+06           NaN   1800000.0   \n",
      "$10,000,000             2.0  1.750000e+07  1.060660e+07  10000000.0   \n",
      "$10,750,000             1.0  1.075000e+07           NaN  10750000.0   \n",
      "...                     ...           ...           ...         ...   \n",
      "Pre-ARB                 4.0  7.749000e+05  2.182384e+04    756000.0   \n",
      "Pre-Arb                 3.0  3.430555e+06  1.929456e+06   2000000.0   \n",
      "UFA                     0.0           NaN           NaN         NaN   \n",
      "Vesting                 9.0  1.792593e+07  4.912506e+06  10000000.0   \n",
      "arbitration-bypassed    1.0  8.400000e+06           NaN   8400000.0   \n",
      "\n",
      "                             25%         50%         75%         max  \n",
      "Status                                                                \n",
      "$1,250,000             1250000.0   1250000.0   1250000.0   1250000.0  \n",
      "$1,750,000             1750000.0   1750000.0   1750000.0   1750000.0  \n",
      "$1,800,000             1800000.0   1800000.0   1800000.0   1800000.0  \n",
      "$10,000,000           13750000.0  17500000.0  21250000.0  25000000.0  \n",
      "$10,750,000           10750000.0  10750000.0  10750000.0  10750000.0  \n",
      "...                          ...         ...         ...         ...  \n",
      "Pre-ARB                 756000.0    774900.0    793800.0    793800.0  \n",
      "Pre-Arb                2333333.0   2666666.0   4145833.0   5625000.0  \n",
      "UFA                          NaN         NaN         NaN         NaN  \n",
      "Vesting               15000000.0  17000000.0  20000000.0  25000000.0  \n",
      "arbitration-bypassed   8400000.0   8400000.0   8400000.0   8400000.0  \n",
      "\n",
      "[98 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Contract Value Calculator\n",
    "Determines player contract values based on:\n",
    "1. Existing payroll data (if available)\n",
    "2. Contract status (Pre-ARB, ARB1-3, FA)\n",
    "3. WAR-based market value\n",
    "\"\"\"\n",
    "\n",
    "# Contract value constants\n",
    "MIN_SALARY = {\n",
    "    'Pre-ARB': 720000,\n",
    "    'ARB1': 1000000,\n",
    "    'ARB2': 2500000,\n",
    "    'ARB3': 4000000\n",
    "}\n",
    "\n",
    "ARB_PERCENT = {\n",
    "    'ARB1': 0.25,\n",
    "    'ARB2': 0.33,\n",
    "    'ARB3': 0.50\n",
    "}\n",
    "\n",
    "def normalize_status(status: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize contract status strings for consistent processing.\n",
    "    \n",
    "    Args:\n",
    "        status: Raw contract status string\n",
    "        \n",
    "    Returns:\n",
    "        Normalized status string (Pre-ARB, ARB1-3, FA, or None)\n",
    "    \"\"\"\n",
    "    if pd.isna(status):\n",
    "        return None\n",
    "        \n",
    "    status = str(status).upper().strip()\n",
    "    \n",
    "    # Handle Pre-ARB variations\n",
    "    if 'PRE' in status and 'ARB' in status:\n",
    "        return 'Pre-ARB'\n",
    "    \n",
    "    # Handle ARB variations\n",
    "    if 'ARB' in status:\n",
    "        for i in range(1, 5):\n",
    "            if str(i) in status:\n",
    "                return f'ARB{min(i, 3)}'  # ARB4 counts as ARB3\n",
    "                \n",
    "    # Handle FA variations\n",
    "    if any(x in status for x in ['UFA', 'FA']):\n",
    "        return 'FA'\n",
    "        \n",
    "    return status\n",
    "\n",
    "def calculate_contract_value(row: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Calculate player's contract value based on status and market value.\n",
    "    \n",
    "    Logic:\n",
    "    1. Use existing Payroll if available\n",
    "    2. Skip FA calculations\n",
    "    3. Apply ARB percentages or minimum salary\n",
    "    \n",
    "    Args:\n",
    "        row: DataFrame row with Status, Payroll, Base_Value, and Year\n",
    "    \n",
    "    Returns:\n",
    "        Contract value for the given year\n",
    "    \"\"\"\n",
    "    # 1. Check for existing Payroll\n",
    "    if pd.notna(row['Payroll']):\n",
    "        return row['Payroll']\n",
    "    \n",
    "    # 2. Get normalized status\n",
    "    status = normalize_status(row['Status'])\n",
    "    \n",
    "    # 3. Handle status-based calculations\n",
    "    if status == 'FA' or status == 'UFA':\n",
    "        return None\n",
    "        \n",
    "    # 4. Calculate minimum salary with inflation\n",
    "    year_offset = row['Year'] - BASE_YEAR\n",
    "    min_salary = MIN_SALARY.get(\n",
    "        status, \n",
    "        MIN_SALARY['Pre-ARB']\n",
    "    ) * (1 + INFLATION_RATE) ** year_offset\n",
    "    \n",
    "    # 5. Handle Pre-ARB\n",
    "    if status == 'Pre-ARB':\n",
    "        return min_salary\n",
    "        \n",
    "    # 6. Handle ARB years\n",
    "    if status in ARB_PERCENT:\n",
    "        return max(\n",
    "            min_salary,\n",
    "            row['Base_Value'] * ARB_PERCENT[status]\n",
    "        )\n",
    "        \n",
    "    # 7. If status exists but isn't handled above, use existing Payroll\n",
    "    return row['Payroll']\n",
    "\n",
    "try:\n",
    "    # Calculate contract values\n",
    "    timeline_with_values = timeline_with_war.copy()\n",
    "    timeline_with_values['Contract_Value'] = timeline_with_values.apply(\n",
    "        calculate_contract_value, \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Calculate surplus value\n",
    "    timeline_with_values['Surplus_Value'] = (\n",
    "        timeline_with_values['Base_Value'] - \n",
    "        timeline_with_values['Contract_Value']\n",
    "    )\n",
    "    \n",
    "    # Validation summary\n",
    "    logger.info(\"\\nContract Value Calculation Summary:\")\n",
    "    logger.info(f\"Total rows processed: {len(timeline_with_values)}\")\n",
    "    logger.info(f\"Rows with contract values: \"\n",
    "               f\"{timeline_with_values['Contract_Value'].notna().sum()}\")\n",
    "    logger.info(\"\\nContract values by status:\")\n",
    "    print(timeline_with_values.groupby('Status')['Contract_Value'].describe())\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to calculate contract values: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-02 17:06:55,522 - INFO - \n",
      "Validation Results:\n",
      "2025-01-02 17:06:55,523 - INFO - Missing data:\n",
      "IDfg                0\n",
      "Year                0\n",
      "Status            262\n",
      "WAR                 6\n",
      "Base_Value          0\n",
      "Contract_Value    700\n",
      "Surplus_Value     700\n",
      "dtype: int64\n",
      "2025-01-02 17:06:55,524 - INFO - Players with invalid progression: 2\n",
      "2025-01-02 17:06:55,525 - INFO - Minimum salary violations: 0\n"
     ]
    }
   ],
   "source": [
    "def validate_salary_timeline(df: pd.DataFrame) -> tuple:\n",
    "    \"\"\"\n",
    "    Validate salary timeline and generate summary statistics.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with calculated values\n",
    "    Returns:\n",
    "        tuple: (missing_data, invalid_progression, min_salary_violations)\n",
    "    \"\"\"\n",
    "    # Data quality checks\n",
    "    validation_cols = ['IDfg', 'Year', 'Status', 'WAR', \n",
    "                      'Base_Value', 'Contract_Value', 'Surplus_Value']\n",
    "    missing_data = df[validation_cols].isnull().sum()\n",
    "    \n",
    "    # Status progression validation\n",
    "    def check_progression(group_df):\n",
    "        ordered_statuses = group_df.sort_values('Year')['Status']\n",
    "        status_sequence = ['Pre-ARB', 'ARB1', 'ARB2', 'ARB3', 'FA']\n",
    "        current_idx = -1\n",
    "        \n",
    "        for status in ordered_statuses:\n",
    "            if status in status_sequence:\n",
    "                new_idx = status_sequence.index(status)\n",
    "                if new_idx <= current_idx:\n",
    "                    return True\n",
    "                current_idx = new_idx\n",
    "        return False\n",
    "    \n",
    "    invalid_progression = df.groupby('IDfg').apply(check_progression)\n",
    "    \n",
    "    # Value thresholds\n",
    "    def check_min_salary(row):\n",
    "        if row['Status'] not in MIN_SALARY:\n",
    "            return False\n",
    "        min_salary = MIN_SALARY[row['Status']] * (1 + INFLATION_RATE) ** (row['Year'] - BASE_YEAR)\n",
    "        return row['Contract_Value'] < min_salary\n",
    "    \n",
    "    min_salary_violations = df[df.apply(check_min_salary, axis=1)]\n",
    "    \n",
    "    # Log validation results\n",
    "    logger.info(\"\\nValidation Results:\")\n",
    "    logger.info(f\"Missing data:\\n{missing_data}\")\n",
    "    logger.info(f\"Players with invalid progression: {invalid_progression.sum()}\")\n",
    "    logger.info(f\"Minimum salary violations: {len(min_salary_violations)}\")\n",
    "    \n",
    "    return missing_data, invalid_progression, min_salary_violations\n",
    "\n",
    "try:\n",
    "    validation_results = validate_salary_timeline(timeline_with_values)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Validation failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-02 17:06:57,217 - INFO - Cleaned timeline data shape: (3387, 11)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Clean duplicate IDfg/Year pairs from timeline data\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    # Sort by number of non-null values (keep rows with most data)\n",
    "    timeline_with_values = (timeline_with_values\n",
    "        .loc[timeline_with_values\n",
    "             .groupby(['IDfg', 'Year'])\n",
    "             .apply(lambda x: x.isnull().sum(axis=1).idxmin())]\n",
    "        .reset_index(drop=True))\n",
    "    \n",
    "    # Verify uniqueness\n",
    "    duplicate_check = timeline_with_values.groupby(['IDfg', 'Year']).size()\n",
    "    if (duplicate_check > 1).any():\n",
    "        raise ValueError(\"Duplicates still exist after cleaning\")\n",
    "        \n",
    "    logger.info(f\"Cleaned timeline data shape: {timeline_with_values.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to clean duplicates: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records processed: 3387\n",
      "Columns: ['Player Name', 'Year', 'Team', 'Payroll', 'Status', 'IDfg', 'FA_Year', 'WAR', 'Base_Value', 'Contract_Value', 'Surplus_Value', 'Age_bat', 'BB%_bat', 'K%_bat', 'AVG', 'OBP', 'SLG', 'wOBA', 'wRC+', 'EV', 'Off', 'BsR', 'Def', 'Age_pit', 'FIP', 'SIERA', 'K%_pit', 'BB%_pit', 'GB%', 'FB%', 'Stuff+', 'Location+', 'Pitching+', 'FBv']\n"
     ]
    }
   ],
   "source": [
    "def integrate_player_statistics(value_data, batter_data, sp_data, rp_data):\n",
    "    \"\"\"Simple integration of stats into value data\"\"\"\n",
    "    \n",
    "    # Start with value data, dropping any duplicate columns\n",
    "    result = value_data.loc[:,~value_data.columns.duplicated()].copy()\n",
    "    \n",
    "    # Prepare batter stats (rename columns to avoid conflicts)\n",
    "    batter_stats = (batter_data[['IDfg', 'prediction_year'] + [col for col in HITTER_COLUMNS if col not in ['Name', 'IDfg', 'WAR']]]\n",
    "                   .rename(columns={\n",
    "                       'prediction_year': 'Year',\n",
    "                       'BB%': 'BB%_bat',\n",
    "                       'K%': 'K%_bat',\n",
    "                       'Age': 'Age_bat'\n",
    "                   }))\n",
    "    \n",
    "    # Prepare pitcher stats (rename columns to avoid conflicts)\n",
    "    pitcher_stats = (pd.concat([\n",
    "        sp_data[['IDfg', 'prediction_year'] + [col for col in PITCHER_COLUMNS if col not in ['Name', 'IDfg', 'WAR']]],\n",
    "        rp_data[['IDfg', 'prediction_year'] + [col for col in PITCHER_COLUMNS if col not in ['Name', 'IDfg', 'WAR']]]\n",
    "    ])\n",
    "    .rename(columns={\n",
    "        'prediction_year': 'Year',\n",
    "        'BB%': 'BB%_pit',\n",
    "        'K%': 'K%_pit',\n",
    "        'Age': 'Age_pit'\n",
    "    })\n",
    "    .drop_duplicates(subset=['IDfg', 'Year']))\n",
    "\n",
    "    # Perform merges with renamed columns\n",
    "    result = (result\n",
    "             .merge(batter_stats, on=['IDfg', 'Year'], how='left')\n",
    "             .merge(pitcher_stats, on=['IDfg', 'Year'], how='left'))\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Execute\n",
    "try:\n",
    "    export_data = integrate_player_statistics(\n",
    "        timeline_with_values,\n",
    "        batter_data,\n",
    "        sp_data, \n",
    "        rp_data\n",
    "    )\n",
    "    print(f\"Records processed: {len(export_data)}\")\n",
    "    print(f\"Columns: {export_data.columns.tolist()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ACTUAL COLUMNS IN EXPORT_DATA:\n",
      "['Player Name', 'Year', 'Team', 'Payroll', 'Status', 'IDfg', 'FA_Year', 'WAR', 'Base_Value', 'Contract_Value', 'Surplus_Value', 'Age_bat', 'BB%_bat', 'K%_bat', 'AVG', 'OBP', 'SLG', 'wOBA', 'wRC+', 'EV', 'Off', 'BsR', 'Def', 'Age_pit', 'FIP', 'SIERA', 'K%_pit', 'BB%_pit', 'GB%', 'FB%', 'Stuff+', 'Location+', 'Pitching+', 'FBv']\n",
      "\n",
      "MISSING COLUMNS:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Let's first verify the actual columns in export_data\n",
    "print(\"\\nACTUAL COLUMNS IN EXPORT_DATA:\")\n",
    "print(export_data.columns.tolist())\n",
    "\n",
    "# Now let's verify if our export columns exist\n",
    "export_cols = [\n",
    "    'Player Name', 'Team', 'Status', 'WAR', \n",
    "    'Base_Value', 'Contract_Value', 'Surplus_Value'\n",
    "]\n",
    "\n",
    "hitting_stats = [\n",
    "    'BB%_bat', 'K%_bat', 'AVG', 'OBP', 'SLG', \n",
    "    'wOBA', 'wRC+', 'EV', 'Off', 'BsR', 'Def'\n",
    "]\n",
    "\n",
    "pitching_stats = [\n",
    "    'FIP', 'SIERA', 'K%_pit', 'BB%_pit', \n",
    "    'GB%', 'FB%', 'Stuff+', 'Location+', 'Pitching+', 'FBv'\n",
    "]\n",
    "\n",
    "# Check which columns are missing\n",
    "print(\"\\nMISSING COLUMNS:\")\n",
    "all_cols = export_cols + hitting_stats + pitching_stats\n",
    "missing = [col for col in all_cols if col not in export_data.columns]\n",
    "print(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-02 16:56:53,338 - INFO - Starting value data export\n",
      "2025-01-02 16:56:53,348 - ERROR - Error processing year 2025.0: \"['Age_bat', 'BB%_bat', 'K%_bat', 'AVG', 'OBP', 'SLG', 'wOBA', 'wRC+', 'EV', 'Off', 'BsR', 'Def', 'Age_pit', 'FIP', 'SIERA', 'K%_pit', 'BB%_pit', 'GB%', 'FB%', 'Stuff+', 'Location+', 'Pitching+', 'FBv'] not in index\"\n",
      "2025-01-02 16:56:53,356 - ERROR - Error processing year 2026.0: \"['Age_bat', 'BB%_bat', 'K%_bat', 'AVG', 'OBP', 'SLG', 'wOBA', 'wRC+', 'EV', 'Off', 'BsR', 'Def', 'Age_pit', 'FIP', 'SIERA', 'K%_pit', 'BB%_pit', 'GB%', 'FB%', 'Stuff+', 'Location+', 'Pitching+', 'FBv'] not in index\"\n",
      "2025-01-02 16:56:53,366 - ERROR - Error processing year 2027.0: \"['Age_bat', 'BB%_bat', 'K%_bat', 'AVG', 'OBP', 'SLG', 'wOBA', 'wRC+', 'EV', 'Off', 'BsR', 'Def', 'Age_pit', 'FIP', 'SIERA', 'K%_pit', 'BB%_pit', 'GB%', 'FB%', 'Stuff+', 'Location+', 'Pitching+', 'FBv'] not in index\"\n",
      "2025-01-02 16:56:53,374 - ERROR - Error processing year 2028.0: \"['Age_bat', 'BB%_bat', 'K%_bat', 'AVG', 'OBP', 'SLG', 'wOBA', 'wRC+', 'EV', 'Off', 'BsR', 'Def', 'Age_pit', 'FIP', 'SIERA', 'K%_pit', 'BB%_pit', 'GB%', 'FB%', 'Stuff+', 'Location+', 'Pitching+', 'FBv'] not in index\"\n",
      "2025-01-02 16:56:53,383 - ERROR - Error processing year 2029.0: \"['Age_bat', 'BB%_bat', 'K%_bat', 'AVG', 'OBP', 'SLG', 'wOBA', 'wRC+', 'EV', 'Off', 'BsR', 'Def', 'Age_pit', 'FIP', 'SIERA', 'K%_pit', 'BB%_pit', 'GB%', 'FB%', 'Stuff+', 'Location+', 'Pitching+', 'FBv'] not in index\"\n",
      "2025-01-02 16:56:53,391 - ERROR - Error processing year 2030.0: \"['Age_bat', 'BB%_bat', 'K%_bat', 'AVG', 'OBP', 'SLG', 'wOBA', 'wRC+', 'EV', 'Off', 'BsR', 'Def', 'Age_pit', 'FIP', 'SIERA', 'K%_pit', 'BB%_pit', 'GB%', 'FB%', 'Stuff+', 'Location+', 'Pitching+', 'FBv'] not in index\"\n",
      "2025-01-02 16:56:53,398 - ERROR - Error processing year 2031.0: \"['Age_bat', 'BB%_bat', 'K%_bat', 'AVG', 'OBP', 'SLG', 'wOBA', 'wRC+', 'EV', 'Off', 'BsR', 'Def', 'Age_pit', 'FIP', 'SIERA', 'K%_pit', 'BB%_pit', 'GB%', 'FB%', 'Stuff+', 'Location+', 'Pitching+', 'FBv'] not in index\"\n",
      "2025-01-02 16:56:53,405 - ERROR - Error processing year 2032.0: \"['Age_bat', 'BB%_bat', 'K%_bat', 'AVG', 'OBP', 'SLG', 'wOBA', 'wRC+', 'EV', 'Off', 'BsR', 'Def', 'Age_pit', 'FIP', 'SIERA', 'K%_pit', 'BB%_pit', 'GB%', 'FB%', 'Stuff+', 'Location+', 'Pitching+', 'FBv'] not in index\"\n",
      "2025-01-02 16:56:53,413 - ERROR - Error processing year 2033.0: \"['Age_bat', 'BB%_bat', 'K%_bat', 'AVG', 'OBP', 'SLG', 'wOBA', 'wRC+', 'EV', 'Off', 'BsR', 'Def', 'Age_pit', 'FIP', 'SIERA', 'K%_pit', 'BB%_pit', 'GB%', 'FB%', 'Stuff+', 'Location+', 'Pitching+', 'FBv'] not in index\"\n",
      "2025-01-02 16:56:53,423 - ERROR - Error processing year 2034.0: \"['Age_bat', 'BB%_bat', 'K%_bat', 'AVG', 'OBP', 'SLG', 'wOBA', 'wRC+', 'EV', 'Off', 'BsR', 'Def', 'Age_pit', 'FIP', 'SIERA', 'K%_pit', 'BB%_pit', 'GB%', 'FB%', 'Stuff+', 'Location+', 'Pitching+', 'FBv'] not in index\"\n",
      "2025-01-02 16:56:53,430 - ERROR - Error processing year 2035.0: \"['Age_bat', 'BB%_bat', 'K%_bat', 'AVG', 'OBP', 'SLG', 'wOBA', 'wRC+', 'EV', 'Off', 'BsR', 'Def', 'Age_pit', 'FIP', 'SIERA', 'K%_pit', 'BB%_pit', 'GB%', 'FB%', 'Stuff+', 'Location+', 'Pitching+', 'FBv'] not in index\"\n",
      "2025-01-02 16:56:53,438 - ERROR - Error processing year 2036.0: \"['Age_bat', 'BB%_bat', 'K%_bat', 'AVG', 'OBP', 'SLG', 'wOBA', 'wRC+', 'EV', 'Off', 'BsR', 'Def', 'Age_pit', 'FIP', 'SIERA', 'K%_pit', 'BB%_pit', 'GB%', 'FB%', 'Stuff+', 'Location+', 'Pitching+', 'FBv'] not in index\"\n",
      "2025-01-02 16:56:53,449 - ERROR - Error processing year 2037.0: \"['Age_bat', 'BB%_bat', 'K%_bat', 'AVG', 'OBP', 'SLG', 'wOBA', 'wRC+', 'EV', 'Off', 'BsR', 'Def', 'Age_pit', 'FIP', 'SIERA', 'K%_pit', 'BB%_pit', 'GB%', 'FB%', 'Stuff+', 'Location+', 'Pitching+', 'FBv'] not in index\"\n",
      "2025-01-02 16:56:53,458 - ERROR - Error processing year 2038.0: \"['Age_bat', 'BB%_bat', 'K%_bat', 'AVG', 'OBP', 'SLG', 'wOBA', 'wRC+', 'EV', 'Off', 'BsR', 'Def', 'Age_pit', 'FIP', 'SIERA', 'K%_pit', 'BB%_pit', 'GB%', 'FB%', 'Stuff+', 'Location+', 'Pitching+', 'FBv'] not in index\"\n",
      "2025-01-02 16:56:53,467 - ERROR - Error processing year 2039.0: \"['Age_bat', 'BB%_bat', 'K%_bat', 'AVG', 'OBP', 'SLG', 'wOBA', 'wRC+', 'EV', 'Off', 'BsR', 'Def', 'Age_pit', 'FIP', 'SIERA', 'K%_pit', 'BB%_pit', 'GB%', 'FB%', 'Stuff+', 'Location+', 'Pitching+', 'FBv'] not in index\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Export Summary:\n",
      "Year\n",
      "2025.0    694\n",
      "2026.0    694\n",
      "2027.0    612\n",
      "2028.0    507\n",
      "2029.0    294\n",
      "2030.0    160\n",
      "2031.0     38\n",
      "2032.0     26\n",
      "2033.0     18\n",
      "2034.0     13\n",
      "2035.0      5\n",
      "2036.0      3\n",
      "2037.0      2\n",
      "2038.0      2\n",
      "2039.0      1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Value Export Module\n",
    "Exports yearly player valuations sorted by team and WAR\n",
    "\"\"\"\n",
    "\n",
    "def export_value_data(df: pd.DataFrame, output_dir: Path) -> None:\n",
    "    \"\"\"Export sorted value data by year, excluding FA players.\"\"\"\n",
    "    logger.info(\"Starting value data export\")\n",
    "    \n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Define all columns to export in correct order\n",
    "    export_cols = [\n",
    "        # Base info\n",
    "        'Player Name', 'Team', 'Status', 'WAR',\n",
    "        'Base_Value', 'Contract_Value', 'Surplus_Value',\n",
    "        \n",
    "        # Hitting stats\n",
    "        'Age_bat', 'BB%_bat', 'K%_bat', 'AVG', 'OBP', 'SLG',\n",
    "        'wOBA', 'wRC+', 'EV', 'Off', 'BsR', 'Def',\n",
    "        \n",
    "        # Pitching stats\n",
    "        'Age_pit', 'FIP', 'SIERA', 'K%_pit', 'BB%_pit',\n",
    "        'GB%', 'FB%', 'Stuff+', 'Location+', 'Pitching+', 'FBv'\n",
    "    ]\n",
    "    \n",
    "    for year in sorted(df['Year'].unique()):\n",
    "        try:\n",
    "            # Get year's data, excluding FA players\n",
    "            year_data = df[\n",
    "                (df['Year'] == year) & \n",
    "                (df['Status'] != 'FA') & \n",
    "                (~df['Status'].str.contains('FA', na=False))\n",
    "            ].copy()\n",
    "            \n",
    "            # Sort by team and WAR\n",
    "            year_data = year_data.sort_values(['Team', 'WAR'], ascending=[True, False])\n",
    "            \n",
    "            # Format numeric columns\n",
    "            numeric_cols = ['Base_Value', 'Contract_Value', 'Surplus_Value']\n",
    "            for col in numeric_cols:\n",
    "                year_data[col] = year_data[col].round(2)\n",
    "            \n",
    "            # Export with all columns\n",
    "            output_file = output_dir / f'player_values_{year}.csv'\n",
    "            year_data[export_cols].to_csv(output_file, index=False)\n",
    "            \n",
    "            logger.info(f\"Exported {len(year_data)} records for {year}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing year {year}: {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    # Filter out FA rows and missing values\n",
    "    export_data = timeline_with_values[\n",
    "        (timeline_with_values['Status'] != 'FA') & \n",
    "        (timeline_with_values['WAR'].notna())\n",
    "    ].copy()\n",
    "    \n",
    "    # Execute export\n",
    "    export_value_data(export_data, OUTPUT_DIR)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nExport Summary:\")\n",
    "    print(export_data.groupby('Year').size())\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Export process failed: {str(e)}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
