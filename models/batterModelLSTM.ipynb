{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import joblib\n",
    "import random\n",
    "import shap  # For feature importance\n",
    "import lime  # For feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load & preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shapes: X=torch.Size([446, 8, 210]), y=torch.Size([446, 210])\n",
      "Validation set shapes: X=torch.Size([127, 8, 210]), y=torch.Size([127, 210])\n",
      "Test set shapes: X=torch.Size([65, 8, 210]), y=torch.Size([65, 210])\n",
      "Scaled target values (first 5 rows):\n",
      "       +WPA      -WPA        1B        2B        3B        AB       AVG  \\\n",
      "0  1.653023 -2.135196  1.887219  0.973764 -0.203547  1.820887  0.433267   \n",
      "1  0.916772 -1.700620  0.947545  0.431230  0.834936  1.445796 -0.337486   \n",
      "2 -0.563070  0.532700 -0.608789 -0.020882 -0.722788 -0.418627 -0.296920   \n",
      "3 -0.404079  0.567235 -0.403235 -0.472994 -0.722788 -0.380015 -0.053524   \n",
      "4 -0.399187  0.311094 -0.403235 -0.111304 -0.722788 -0.363467  0.068173   \n",
      "\n",
      "       AVG+       Age     BABIP  ...  wSI (pi)  wSI (sc)  wSI/C (pi)  \\\n",
      "0  0.547906  1.227872  0.062676  ... -0.723554 -0.565805   -0.090467   \n",
      "1 -0.068996  1.483056 -0.811781  ... -1.070153 -1.011411   -0.166014   \n",
      "2 -0.171813  1.227872 -0.260493  ... -0.376956 -0.518900   -0.458126   \n",
      "3  0.188046  1.483056 -0.279503  ... -0.975626 -1.011411   -1.007096   \n",
      "4  0.188046  0.717505 -0.393563  ... -1.133170 -0.964505   -0.881185   \n",
      "\n",
      "   wSI/C (sc)       wSL  wSL (pi)  wSL (sc)     wSL/C  wSL/C (pi)  wSL/C (sc)  \n",
      "0   -0.038064 -3.071211 -4.402052 -3.701996 -0.336587   -0.692848   -0.430736  \n",
      "1   -0.178012 -1.906603 -3.075673 -1.884264 -0.166101   -0.553063   -0.148155  \n",
      "2   -0.567244 -0.683765 -0.323437 -0.564943 -0.247082   -0.069193   -0.186689  \n",
      "3   -1.162025 -0.101461 -0.257118 -0.125169  0.136511   -0.042311    0.130144  \n",
      "4   -0.768420  0.102346  0.008158  0.138696  0.268637    0.140485    0.297124  \n",
      "\n",
      "[5 rows x 210 columns]\n",
      "\n",
      "Unscaled target values (first 5 rows):\n",
      "    +WPA   -WPA     1B    2B            3B     AB    AVG   AVG+   Age  BABIP  \\\n",
      "0  12.28 -12.96  111.0  25.0  1.000000e+00  613.0  0.258  104.0  33.0  0.291   \n",
      "1   9.27 -11.45   79.0  19.0  3.000000e+00  545.0  0.220   92.0  34.0  0.245   \n",
      "2   3.22  -3.69   26.0  14.0  1.730128e-08  207.0  0.222   90.0  33.0  0.274   \n",
      "3   3.87  -3.57   33.0   9.0  1.730128e-08  214.0  0.234   97.0  34.0  0.273   \n",
      "4   3.89  -4.46   33.0  13.0  1.730128e-08  217.0  0.240   97.0  31.0  0.267   \n",
      "\n",
      "   ...  wSI (pi)  wSI (sc)  wSI/C (pi)  wSI/C (sc)   wSL  wSL (pi)  wSL (sc)  \\\n",
      "0  ...      -2.4 -2.200000       -0.53       -0.48 -11.2     -13.8     -13.4   \n",
      "1  ...      -3.5 -4.099999       -0.68       -0.80  -7.2      -9.8      -7.2   \n",
      "2  ...      -1.3 -2.000000       -1.26       -1.69  -3.0      -1.5      -2.7   \n",
      "3  ...      -3.2 -4.099999       -2.35       -3.05  -1.0      -1.3      -1.2   \n",
      "4  ...      -3.7 -3.900000       -2.10       -2.15  -0.3      -0.5      -0.3   \n",
      "\n",
      "   wSL/C  wSL/C (pi)  wSL/C (sc)  \n",
      "0  -1.57       -1.80       -1.86  \n",
      "1  -1.17       -1.54       -1.20  \n",
      "2  -1.36       -0.64       -1.29  \n",
      "3  -0.46       -0.59       -0.55  \n",
      "4  -0.15       -0.25       -0.16  \n",
      "\n",
      "[5 rows x 210 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "def load_data(file_path):\n",
    "    return pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "# Convert columns to appropriate types\n",
    "def convert_column_types(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'int64':\n",
    "            df[col] = df[col].astype('int32')\n",
    "        elif df[col].dtype == 'float64':\n",
    "            df[col] = df[col].astype('float32')\n",
    "    return df\n",
    "\n",
    "#define function that saves a csv matching IDfg with player name\n",
    "def save_player_names(df):\n",
    "    player_names = df[['IDfg', 'Name']].drop_duplicates()\n",
    "    player_names.to_csv('../data/player_names.csv', index=False)\n",
    "\n",
    "# Filter data for specific seasons, players with sufficient AB, and PA >= 50\n",
    "def filter_data(df, start_season=2010, min_pa=50):\n",
    "    df = df[df['Season'] >= start_season]\n",
    "    df = df[df['PA'] >= min_pa]\n",
    "    return df\n",
    "\n",
    "def drop_cols(df):\n",
    "    df.dropna(axis=1, inplace=True)  # Remove columns with any missing values\n",
    "    # Drop specific columns\n",
    "    df.drop(columns=['Age Rng', 'Dol', 'Name'], inplace=True)\n",
    "    \n",
    "    # One-hot encode string columns and store the mapping\n",
    "    string_columns = df.select_dtypes(include=['object']).columns\n",
    "    one_hot_mapping = {}\n",
    "    for col in string_columns:\n",
    "        one_hot_mapping[col] = df[col].unique().tolist()\n",
    "    df = pd.get_dummies(df, columns=string_columns, drop_first=True)\n",
    "    \n",
    "    return df, one_hot_mapping\n",
    "\n",
    "# Scale both input and target features together (to avoid double scaling)\n",
    "def scale_features(df, features, scaler=None):\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        df[features] = scaler.fit_transform(df[features])\n",
    "        joblib.dump(scaler, 'scaler.pkl')  # Save the scaler for future use\n",
    "    else:\n",
    "        df[features] = scaler.transform(df[features])\n",
    "    \n",
    "    return df, scaler\n",
    "\n",
    "\n",
    "# Create sequences for model input\n",
    "def create_sequences(df, input_features, target_features, seq_length=3):\n",
    "    sequences = []\n",
    "    grouped_data = df.groupby('IDfg')\n",
    "\n",
    "    for _, player_data in grouped_data:\n",
    "        player_data = player_data.sort_values(by='Season')\n",
    "        input_data = player_data[input_features]\n",
    "        target_data = player_data[target_features]\n",
    "\n",
    "        for i in range(len(input_data) - seq_length):\n",
    "            seq = input_data.iloc[i:i + seq_length].values\n",
    "            label = target_data.iloc[i + seq_length].values\n",
    "            sequences.append((seq, label))\n",
    "\n",
    "    return sequences\n",
    "\n",
    "# Convert sequences to PyTorch tensors\n",
    "def to_tensor(sequences):\n",
    "    X = torch.tensor([s[0] for s in sequences], dtype=torch.float32)\n",
    "    y = torch.tensor([s[1] for s in sequences], dtype=torch.float32)\n",
    "    return X, y\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "def split_data(sequences, train_ratio=0.7, valid_ratio=0.2):\n",
    "    train_size = int(len(sequences) * train_ratio)\n",
    "    valid_size = int(len(sequences) * valid_ratio)\n",
    "    test_size = len(sequences) - train_size - valid_size\n",
    "\n",
    "    train_sequences = sequences[:train_size]\n",
    "    valid_sequences = sequences[train_size:train_size + valid_size]\n",
    "    test_sequences = sequences[train_size + valid_size:]\n",
    "\n",
    "    return train_sequences, valid_sequences, test_sequences\n",
    "\n",
    "def invert_one_hot_encoding(df, one_hot_mapping):\n",
    "    for col, categories in one_hot_mapping.items():\n",
    "        # Find the one-hot encoded columns for this original column\n",
    "        one_hot_cols = [f\"{col}_{category}\" for category in categories if f\"{col}_{category}\" in df.columns]\n",
    "        \n",
    "        # Create a new column with the original categorical values\n",
    "        df[col] = df[one_hot_cols].idxmax(axis=1).apply(lambda x: x.split('_', 1)[1])\n",
    "        \n",
    "        # Drop the one-hot encoded columns\n",
    "        df.drop(columns=one_hot_cols, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Modify preprocess_data to scale features in one step\n",
    "def preprocess_data(file_path, seq_length=3, start_season=2010, min_pa=50):\n",
    "    # Load and clean data\n",
    "    df = load_data(file_path)\n",
    "    # Save player names\n",
    "    save_player_names(df)\n",
    "    df = filter_data(df, start_season=start_season, min_pa=min_pa)\n",
    "    \n",
    "    # Drop columns with any missing values and one-hot encode string columns\n",
    "    df, one_hot_mapping = drop_cols(df)\n",
    "\n",
    "    # Convert column types\n",
    "    df = convert_column_types(df)\n",
    "\n",
    "    # Define input and target features as all columns except 'IDfg' and 'Season'\n",
    "    input_features = df.columns.difference(['IDfg', 'Season']).tolist()\n",
    "    target_features = input_features\n",
    "\n",
    "    # Scale input and target features in one step\n",
    "    df, scaler = scale_features(df, input_features)\n",
    "\n",
    "    # Create sequences\n",
    "    sequences = create_sequences(df, input_features, target_features, seq_length=seq_length)\n",
    "\n",
    "    # Split sequences into train, validation, and test sets\n",
    "    train_sequences, valid_sequences, test_sequences = split_data(sequences)\n",
    "\n",
    "    # Convert to tensors\n",
    "    X_train, y_train = to_tensor(train_sequences)\n",
    "    X_valid, y_valid = to_tensor(valid_sequences)\n",
    "    X_test, y_test = to_tensor(test_sequences)\n",
    "\n",
    "    # Return preprocessed tensors, scaler, one-hot mapping, and processed DataFrame\n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test, scaler, one_hot_mapping, df\n",
    "\n",
    "# Preprocess the data\n",
    "file_path = '../data/mlb_batting_data_2010_2024.csv'\n",
    "seq_length = 8\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test, scaler, one_hot_mapping, df = preprocess_data(file_path, seq_length=seq_length, start_season=2010, min_pa=50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.5):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        # Batch normalization layer\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.fc2 = nn.Linear(hidden_size // 2, output_size)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def attention_net(self, lstm_output, final_hidden_state):\n",
    "        # Applying attention on the hidden state at each time step\n",
    "        attn_weights = torch.tanh(self.attention(lstm_output))\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
    "\n",
    "        # Multiply attention weights with the LSTM output to get weighted sum\n",
    "        context_vector = torch.sum(attn_weights * lstm_output, dim=1)\n",
    "        return context_vector\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # Initialize hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Pack the padded sequence\n",
    "        packed_input = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input, (h0, c0))\n",
    "        \n",
    "        # Unpack the sequence\n",
    "        lstm_output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        # Apply attention mechanism to weigh different time steps\n",
    "        context_vector = self.attention_net(lstm_output, hn[-1])\n",
    "        \n",
    "        # Apply batch normalization\n",
    "        context_vector = self.batch_norm(context_vector)\n",
    "        \n",
    "        # Fully connected layers with dropout\n",
    "        out = self.fc1(context_vector)\n",
    "        out = torch.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=10, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss >= self.best_loss:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    sequences, targets = zip(*batch)\n",
    "\n",
    "    # Convert sequences to PyTorch tensors and pad them\n",
    "    sequences_padded = pad_sequence([seq.clone().detach() for seq in sequences], batch_first=True, padding_value=0)\n",
    "\n",
    "    # Convert targets to a single tensor\n",
    "    targets_padded = torch.stack([target.clone().detach() for target in targets])\n",
    "\n",
    "    # Get lengths\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences], dtype=torch.long)\n",
    "\n",
    "    return sequences_padded, lengths, targets_padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_projected_vs_actual(model, X_train, y_train, target_scaler, num_samples=5):\n",
    "    \"\"\"\n",
    "    Print a comparison between the projected and actual stats for a few samples.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model used for prediction.\n",
    "        X_train: Tensor containing the training feature sequences.\n",
    "        y_train: Tensor containing the training target values.\n",
    "        target_scaler: Scaler used to inverse transform the scaled target data.\n",
    "        num_samples: Number of samples to display for comparison.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Select a few random samples to compare\n",
    "        sample_indices = torch.randint(0, X_train.size(0), (num_samples,))\n",
    "\n",
    "        for idx in sample_indices:\n",
    "            # Extract the feature sequence and true target\n",
    "            seq = X_train[idx].unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "            true_target = y_train[idx].unsqueeze(0).to(device)\n",
    "\n",
    "            # Predict the stats\n",
    "            lengths = torch.tensor([seq.size(1)], dtype=torch.int64).cpu()  # Sequence length, move lengths to CPU\n",
    "            predicted_target = model(seq, lengths)\n",
    "\n",
    "            # Since predicted_target and true_target are tensors with shape (1, num_features)\n",
    "            # We need to ensure they match the expected shape for inverse_transform\n",
    "            predicted_target = predicted_target.squeeze().cpu().numpy()\n",
    "            true_target = true_target.squeeze().cpu().numpy()\n",
    "\n",
    "            # Print shapes for debugging\n",
    "            print(f\"Predicted target shape: {predicted_target.shape}\")\n",
    "            print(f\"True target shape: {true_target.shape}\")\n",
    "            print(f\"Scaler features: {target_scaler.n_features_in_}\")\n",
    "\n",
    "            # Ensure correct shape for scaler\n",
    "            if predicted_target.shape[0] != target_scaler.n_features_in_:\n",
    "                raise ValueError(f\"Predicted target shape {predicted_target.shape} does not match scaler features {target_scaler.n_features_in_}\")\n",
    "\n",
    "            # Inverse transform the predicted and actual values\n",
    "            predicted_target = target_scaler.inverse_transform(predicted_target.reshape(1, -1))\n",
    "            true_target = target_scaler.inverse_transform(true_target.reshape(1, -1))\n",
    "\n",
    "            # Display the comparison\n",
    "            print(f\"\\nSample {idx.item() + 1}:\")\n",
    "            print(f\"Actual Stats:    G={true_target[0][0]:.2f}, HR={true_target[0][1]:.2f}, OBP={true_target[0][2]:.3f}, \"\n",
    "                  f\"SLG={true_target[0][3]:.3f}, WAR={true_target[0][4]:.2f}\")\n",
    "            print(f\"Predicted Stats: G={predicted_target[0][0]:.2f}, HR={predicted_target[0][1]:.2f}, OBP={predicted_target[0][2]:.3f}, \"\n",
    "                  f\"SLG={predicted_target[0][3]:.3f}, WAR={predicted_target[0][4]:.2f}\")\n",
    "\n",
    "    model.train()  # Switch back to training mode after evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "input_size = X_train.shape[2]  # Number of features\n",
    "hidden_size = 128  # Hidden size for complexity\n",
    "num_layers = 2  # Number of layers (not too deep)\n",
    "batch_size = 32\n",
    "output_size = y_train.shape[1]  # Number of target features\n",
    "dropout = 0.4\n",
    "criterion = nn.MSELoss()  # Loss function\n",
    "\n",
    "# Initialize the model\n",
    "model = LSTM(input_size, hidden_size, num_layers, output_size, dropout).to(device)\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.01, weight_decay=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)  # Learning rate scheduler\n",
    "\n",
    "# Early stopping setup\n",
    "early_stopping = EarlyStopping(patience=10, verbose=True)\n",
    "\n",
    "# Create DataLoader for training and validation\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "print_interval = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 0.8697\n",
      "Epoch 1/100, Validation Loss: 0.8617\n",
      "Epoch 2/100, Training Loss: 0.7882\n",
      "Epoch 2/100, Validation Loss: 0.8543\n",
      "Epoch 3/100, Training Loss: 0.7478\n",
      "Epoch 3/100, Validation Loss: 0.8076\n",
      "Epoch 4/100, Training Loss: 0.7270\n",
      "Epoch 4/100, Validation Loss: 0.8020\n",
      "Epoch 5/100, Training Loss: 0.6997\n",
      "Epoch 5/100, Validation Loss: 0.8055\n",
      "Epoch 6/100, Training Loss: 0.7058\n",
      "Epoch 6/100, Validation Loss: 0.9118\n",
      "Epoch 7/100, Training Loss: 0.7019\n",
      "Epoch 7/100, Validation Loss: 0.8255\n",
      "Epoch 8/100, Training Loss: 0.6752\n",
      "Epoch 8/100, Validation Loss: 0.8515\n",
      "Epoch 9/100, Training Loss: 0.6764\n",
      "Epoch 9/100, Validation Loss: 0.8185\n",
      "Epoch 10/100, Training Loss: 0.6587\n",
      "Epoch 10/100, Validation Loss: 0.8077\n",
      "Epoch 11/100, Training Loss: 0.6355\n",
      "Epoch 11/100, Validation Loss: 0.8000\n",
      "Epoch 12/100, Training Loss: 0.6243\n",
      "Epoch 12/100, Validation Loss: 0.8192\n",
      "Epoch 13/100, Training Loss: 0.6182\n",
      "Epoch 13/100, Validation Loss: 0.8180\n",
      "Epoch 14/100, Training Loss: 0.6080\n",
      "Epoch 14/100, Validation Loss: 0.8128\n",
      "Epoch 15/100, Training Loss: 0.5986\n",
      "Epoch 15/100, Validation Loss: 0.8390\n",
      "Epoch 16/100, Training Loss: 0.5995\n",
      "Epoch 16/100, Validation Loss: 0.8283\n",
      "Epoch 17/100, Training Loss: 0.5905\n",
      "Epoch 17/100, Validation Loss: 0.8208\n",
      "Epoch 18/100, Training Loss: 0.5994\n",
      "Epoch 18/100, Validation Loss: 0.8197\n",
      "Epoch 19/100, Training Loss: 0.5841\n",
      "Epoch 19/100, Validation Loss: 0.8286\n",
      "Epoch 20/100, Training Loss: 0.5840\n",
      "Epoch 20/100, Validation Loss: 0.8443\n",
      "Epoch 21/100, Training Loss: 0.5778\n",
      "Epoch 21/100, Validation Loss: 0.8296\n",
      "Early stopping\n",
      "Model saved to lstm_model.pth\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch_X, lengths, batch_y in train_loader:\n",
    "        batch_X = batch_X.to(device)  # Move input features to GPU\n",
    "        batch_y = batch_y.to(device)  # Move targets to GPU\n",
    "        lengths = lengths.to(\"cpu\")  # Ensure lengths are on CPU for packing/unpacking sequences\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_X, lengths)\n",
    "        \n",
    "        # Ensure outputs and targets have matching shapes\n",
    "        if outputs.shape != batch_y.shape:\n",
    "            raise ValueError(f\"Shape mismatch: outputs shape {outputs.shape}, target shape {batch_y.shape}\")\n",
    "        \n",
    "        # Loss calculation for multi-target regression\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for val_batch_X, val_lengths, val_batch_y in valid_loader:\n",
    "            val_batch_X = val_batch_X.to(device)  # Move validation features to GPU\n",
    "            val_batch_y = val_batch_y.to(device)  # Move validation targets to GPU\n",
    "            val_lengths = val_lengths.to(\"cpu\")  # Ensure lengths are on CPU for packing/unpacking sequences\n",
    "            \n",
    "            val_outputs = model(val_batch_X, val_lengths)\n",
    "            \n",
    "            # Ensure validation outputs and targets match in shape\n",
    "            if val_outputs.shape != val_batch_y.shape:\n",
    "                raise ValueError(f\"Shape mismatch: validation outputs shape {val_outputs.shape}, target shape {val_batch_y.shape}\")\n",
    "            \n",
    "            val_loss += criterion(val_outputs, val_batch_y).item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(valid_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    # Check early stopping\n",
    "    early_stopping(avg_val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "    # Adjust learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "\n",
    "# Save the trained model\n",
    "model_save_path = 'lstm_model.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f'Model saved to {model_save_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on individual players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../mlb_batting_data_1912_2023.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 103\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[0;32m    102\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../mlb_batting_data_1912_2023.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 103\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m df \u001b[38;5;241m=\u001b[39m convert_column_types(df)\n\u001b[0;32m    105\u001b[0m df \u001b[38;5;241m=\u001b[39m filter_data(df, start_season\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m, min_ab\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "Cell \u001b[1;32mIn[24], line 3\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(file_path):\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../mlb_batting_data_1912_2023.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Define the manual unscale function\n",
    "def manual_unscale(y_scaled, scaler, target_feature_indices=None):\n",
    "    if target_feature_indices is not None:\n",
    "        # Extract the relevant mean and scale for the target columns\n",
    "        target_means = scaler.mean_[target_feature_indices]\n",
    "        target_scales = scaler.scale_[target_feature_indices]\n",
    "\n",
    "        # Manually apply inverse transformation\n",
    "        y_unscaled = (y_scaled * target_scales) + target_means\n",
    "        return y_unscaled\n",
    "\n",
    "    return scaler.inverse_transform(y_scaled)\n",
    "\n",
    "# Function to choose a random player\n",
    "def choose_random_player(df):\n",
    "    eligible_players = df[df['Season'] == 2023].groupby('IDfg').filter(lambda x: x['AB'].sum() >= 100)['IDfg'].unique()\n",
    "    return np.random.choice(eligible_players)\n",
    "\n",
    "# Function to gather all years of player data\n",
    "def gather_player_data(df, player_id):\n",
    "    player_data = df[df['IDfg'] == player_id].sort_values(by='Season')\n",
    "    return player_data\n",
    "\n",
    "# Function to predict future stats\n",
    "def predict_future_stats(player_id, input_features, target_features, model, scaler, df, one_hot_mapping):\n",
    "    # Gather all years of player data\n",
    "    player_data = gather_player_data(df, player_id)\n",
    "    if player_data.empty:\n",
    "        print(f\"No data found for player ID {player_id}.\")\n",
    "        return\n",
    "\n",
    "    last_known_data = player_data[input_features].iloc[-1].values\n",
    "    player_name = player_data['Name'].iloc[0]  # Assuming 'Name' column exists\n",
    "\n",
    "    # Print player's historical data\n",
    "    print(f\"\\nPlayer: {player_name}\\n\")\n",
    "    print(f\"Previous stats:\")\n",
    "    for _, row in player_data.iterrows():\n",
    "        print(f\"Year {row['Season']} - Age {row['Age']}: G: {row['G']}, HR: {row['HR']}, OBP: {row['OBP']:.3f}, SLG: {row['SLG']:.3f}, WAR: {row['WAR']:.3f}\")\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    print(\"\\nPredicted future stats (2024-2026):\")\n",
    "    for year in range(2024, 2027):\n",
    "        # Calculate the correct age\n",
    "        current_age = last_known_data[0] + 1\n",
    "        \n",
    "        # Prepare input data for the model\n",
    "        input_data = last_known_data.reshape(1, -1)\n",
    "\n",
    "        # Scale the input data\n",
    "        scaled_input = scaler.transform(input_data)\n",
    "\n",
    "        # Convert to tensor and predict\n",
    "        input_tensor = torch.tensor(scaled_input, dtype=torch.float32).to(device)\n",
    "        predicted_stats = model(input_tensor.unsqueeze(1), torch.tensor([1], dtype=torch.int64).cpu())\n",
    "        predicted_stats = predicted_stats.squeeze().cpu().detach().numpy()\n",
    "\n",
    "        # Ensure the predictions are in the right shape for inverse scaling\n",
    "        predicted_stats_reshaped = predicted_stats.reshape(1, -1)\n",
    "\n",
    "        # Manually unscale the predictions\n",
    "        target_feature_indices = [input_features.index(f) for f in target_features]\n",
    "        unscaled_predictions = manual_unscale(predicted_stats_reshaped, scaler, target_feature_indices)\n",
    "\n",
    "        # Replace the predicted age with the correct age\n",
    "        unscaled_predictions[0][0] = current_age\n",
    "\n",
    "        # Store the predictions\n",
    "        predictions.append({\n",
    "            'Year': year,\n",
    "            'Age': current_age,\n",
    "            'G': unscaled_predictions[0][1],\n",
    "            'HR': unscaled_predictions[0][2],\n",
    "            'OBP': unscaled_predictions[0][3],\n",
    "            'SLG': unscaled_predictions[0][4],\n",
    "            'WAR': unscaled_predictions[0][5]\n",
    "        })\n",
    "        \n",
    "        # Update last_known_data for the next iteration\n",
    "        last_known_data = np.array([\n",
    "            current_age,\n",
    "            unscaled_predictions[0][1],\n",
    "            unscaled_predictions[0][2],\n",
    "            unscaled_predictions[0][3],\n",
    "            unscaled_predictions[0][4],\n",
    "            unscaled_predictions[0][5]  # Update WAR with the predicted value\n",
    "        ])\n",
    "\n",
    "    # Print the predictions\n",
    "    for prediction in predictions:\n",
    "        print(f\"Year {prediction['Year']} - Age {prediction['Age']}:\")\n",
    "        print(f\"G: {prediction['G']:.2f}, HR: {prediction['HR']:.2f}, OBP: {prediction['OBP']:.3f}, SLG: {prediction['SLG']:.3f}, WAR: {prediction['WAR']:.3f}\\n\")\n",
    "# Preprocess the data\n",
    "file_path = '../data/mlb_batting_data_2010_2024.csv'\n",
    "seq_length = 8\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test, scaler, one_hot_mapping, processed_df = preprocess_data(file_path, seq_length=seq_length, start_season=2010, min_pa=50)\n",
    "\n",
    "# Print shapes for debugging\n",
    "print(f\"Train set shapes: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Validation set shapes: X={X_valid.shape}, y={y_valid.shape}\")\n",
    "print(f\"Test set shapes: X={X_test.shape}, y={y_test.shape}\")\n",
    "\n",
    "# Get a random player\n",
    "player_id = choose_random_player(processed_df)\n",
    "\n",
    "# Define input and target features\n",
    "input_features = processed_df.columns.difference(['IDfg', 'Season']).tolist()\n",
    "target_features = input_features\n",
    "\n",
    "# Load scaler\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "print(f\"Scaler mean: {scaler.mean_}, scale: {scaler.scale_}\")\n",
    "\n",
    "# Predict future stats for the chosen player\n",
    "predict_future_stats(player_id, input_features, target_features, model, scaler, processed_df, one_hot_mapping)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
