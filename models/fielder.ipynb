{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Data processing and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass, asdict, field\n",
    "\n",
    "\n",
    "\n",
    "# Progress tracking and logging\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# Random seed\n",
    "import random\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Total valid rows after cleaning: 10453\n",
      "INFO:__main__:\n",
      "Value ranges:\n",
      "INFO:__main__:Inn: 0.1 to 1443.1\n",
      "INFO:__main__:DRS: -27.0 to 41.0\n",
      "INFO:__main__:UZR/150: -259.2 to 222.6\n",
      "INFO:__main__:OAA: -23.0 to 35.0\n",
      "INFO:__main__:Age: 19.0 to 44.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 677\n",
      "Valid size: 193\n",
      "Test size: 98\n",
      "\n",
      "Batch shapes:\n",
      "history: torch.Size([32, 3, 9, 4])\n",
      "history_mask: torch.Size([32, 3, 9])\n",
      "target: torch.Size([32, 9, 4])\n",
      "target_mask: torch.Size([32, 9])\n",
      "age: torch.Size([32, 3])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    seq_length: int = 3\n",
    "    start_season: int = 2016\n",
    "    train_ratio: float = 0.7\n",
    "    valid_ratio: float = 0.2\n",
    "    batch_size: int = 32\n",
    "    min_innings: float = 200.0\n",
    "    positions: List[str] = field(default_factory=lambda: ['C', '1B', '2B', '3B', 'SS', 'LF', 'CF', 'RF', 'DH'])\n",
    "\n",
    "@dataclass\n",
    "class PositionalMetrics:\n",
    "    inn: float\n",
    "    drs: float\n",
    "    uzr: float\n",
    "    oaa: float\n",
    "    is_primary: bool = False\n",
    "\n",
    "@dataclass\n",
    "class SeasonSnapshot:\n",
    "    season: int\n",
    "    age: int\n",
    "    metrics: Dict[str, PositionalMetrics]\n",
    "\n",
    "@dataclass\n",
    "class PlayerSequence:\n",
    "    player_id: str\n",
    "    history: List[SeasonSnapshot]\n",
    "    target: Optional[SeasonSnapshot] = None\n",
    "\n",
    "class FieldingDataset(Dataset):\n",
    "    def __init__(self, sequences: List[PlayerSequence], positions: List[str]):\n",
    "        self.sequences = sequences\n",
    "        self.positions = positions\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        \n",
    "        # Create tensors with explicit dtype\n",
    "        history_tensor = torch.zeros(len(sequence.history), len(self.positions), 4, dtype=torch.float32)\n",
    "        position_mask = torch.zeros(len(sequence.history), len(self.positions), dtype=torch.float32)\n",
    "        \n",
    "        # Fill history tensor\n",
    "        for t, season in enumerate(sequence.history):\n",
    "            for pos_idx, pos in enumerate(self.positions):\n",
    "                if pos in season.metrics:\n",
    "                    metrics = season.metrics[pos]\n",
    "                    history_tensor[t, pos_idx] = torch.tensor([\n",
    "                        metrics.inn, metrics.drs, metrics.uzr, metrics.oaa\n",
    "                    ], dtype=torch.float32)\n",
    "                    position_mask[t, pos_idx] = 1\n",
    "        \n",
    "        # Create target tensor if exists\n",
    "        if sequence.target:\n",
    "            target_tensor = torch.zeros(len(self.positions), 4, dtype=torch.float32)\n",
    "            target_mask = torch.zeros(len(self.positions), dtype=torch.float32)\n",
    "            \n",
    "            for pos_idx, pos in enumerate(self.positions):\n",
    "                if pos in sequence.target.metrics:\n",
    "                    metrics = sequence.target.metrics[pos]\n",
    "                    target_tensor[pos_idx] = torch.tensor([\n",
    "                        metrics.inn, metrics.drs, metrics.uzr, metrics.oaa\n",
    "                    ], dtype=torch.float32)\n",
    "                    target_mask[pos_idx] = 1\n",
    "        else:\n",
    "            target_tensor = torch.zeros(len(self.positions), 4, dtype=torch.float32)\n",
    "            target_mask = torch.zeros(len(self.positions), dtype=torch.float32)\n",
    "            \n",
    "        return {\n",
    "            'history': history_tensor,\n",
    "            'history_mask': position_mask,\n",
    "            'target': target_tensor,\n",
    "            'target_mask': target_mask,\n",
    "            'age': torch.tensor([s.age for s in sequence.history], dtype=torch.float32),\n",
    "            'player_id': sequence.player_id\n",
    "        }\n",
    "\n",
    "def prepare_sequences(df: pd.DataFrame, config: DataConfig) -> List[PlayerSequence]:\n",
    "    sequences = []\n",
    "    \n",
    "    # Filter required columns first\n",
    "    required_columns = ['Inn', 'DRS', 'UZR/150', 'OAA', 'Pos', 'IDfg', 'Season', 'Age']\n",
    "    df = df[\n",
    "        df[required_columns].notna().all(axis=1) &  # Remove rows with any NaN\n",
    "        (df['Inn'] > 0) &  # Remove invalid innings\n",
    "        df['Inn'].notna()  # Ensure innings are valid\n",
    "    ]\n",
    "    \n",
    "    # Replace infinities and verify no NaN values remain\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=['Inn', 'DRS', 'UZR/150', 'OAA'])\n",
    "    \n",
    "    # Log data quality\n",
    "    logger.info(f\"Total valid rows after cleaning: {len(df)}\")\n",
    "    logger.info(\"\\nValue ranges:\")\n",
    "    for col in ['Inn', 'DRS', 'UZR/150', 'OAA', 'Age']:\n",
    "        logger.info(f\"{col}: {df[col].min():.1f} to {df[col].max():.1f}\")\n",
    "    \n",
    "    for player_id, player_data in df.groupby('IDfg'):\n",
    "        player_data = player_data.sort_values('Season')\n",
    "        seasons = []\n",
    "        \n",
    "        # Group by season\n",
    "        for season, season_data in player_data.groupby('Season'):\n",
    "            # Get primary position (most innings)\n",
    "            primary_pos = season_data.loc[season_data['Inn'].idxmax(), 'Pos']\n",
    "            \n",
    "            # Create position metrics dictionary\n",
    "            position_metrics = {}\n",
    "            for _, row in season_data.iterrows():\n",
    "                if row['Inn'] >= config.min_innings:\n",
    "                    position_metrics[row['Pos']] = PositionalMetrics(\n",
    "                        inn=float(row['Inn']),  # Ensure float type\n",
    "                        drs=float(row['DRS']),\n",
    "                        uzr=float(row['UZR/150']),\n",
    "                        oaa=float(row['OAA']),\n",
    "                        is_primary=(row['Pos'] == primary_pos)\n",
    "                    )\n",
    "            \n",
    "            if position_metrics:  # Only add season if we have valid positions\n",
    "                seasons.append(SeasonSnapshot(\n",
    "                    season=int(season),  # Ensure int type\n",
    "                    age=int(season_data['Age'].iloc[0]),\n",
    "                    metrics=position_metrics\n",
    "                ))\n",
    "        \n",
    "        # Create sequences with sliding window\n",
    "        for i in range(len(seasons) - config.seq_length):\n",
    "            history = seasons[i:i+config.seq_length]\n",
    "            target = seasons[i+config.seq_length]\n",
    "            sequences.append(PlayerSequence(\n",
    "                player_id=str(player_id),  # Ensure string type\n",
    "                history=history,\n",
    "                target=target\n",
    "            ))\n",
    "    \n",
    "    return sequences\n",
    "# Load and process data\n",
    "config = DataConfig()\n",
    "\n",
    "# Read data\n",
    "fielding_df = pd.read_csv('../data/mlb_fielding_data_2000_2024.csv')\n",
    "batting_df = pd.read_csv('../data/mlb_batting_data_2010_2024.csv')\n",
    "pitching_df = pd.read_csv('../data/mlb_pitching_data_2000_2024.csv')\n",
    "\n",
    "# Merge age information\n",
    "age_lookup = pd.concat([\n",
    "    batting_df[['IDfg', 'Season', 'Age']],\n",
    "    pitching_df[['IDfg', 'Season', 'Age']]\n",
    "]).drop_duplicates()\n",
    "\n",
    "df = fielding_df.merge(\n",
    "    age_lookup,\n",
    "    on=['IDfg', 'Season'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Create sequences\n",
    "sequences = prepare_sequences(df, config)\n",
    "\n",
    "# Split into train/valid/test\n",
    "total_size = len(sequences)\n",
    "train_size = int(total_size * config.train_ratio)\n",
    "valid_size = int(total_size * config.valid_ratio)\n",
    "\n",
    "indices = np.random.permutation(total_size)\n",
    "train_indices = indices[:train_size]\n",
    "valid_indices = indices[train_size:train_size + valid_size]\n",
    "test_indices = indices[train_size + valid_size:]\n",
    "\n",
    "train_sequences = [sequences[i] for i in train_indices]\n",
    "valid_sequences = [sequences[i] for i in valid_indices]\n",
    "test_sequences = [sequences[i] for i in test_indices]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = FieldingDataset(train_sequences, config.positions)\n",
    "valid_dataset = FieldingDataset(valid_sequences, config.positions)\n",
    "test_dataset = FieldingDataset(test_sequences, config.positions)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=config.batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.batch_size)\n",
    "\n",
    "# Print dataset sizes and inspect a batch\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Valid size: {len(valid_dataset)}\")\n",
    "print(f\"Test size: {len(test_dataset)}\")\n",
    "\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(\"\\nBatch shapes:\")\n",
    "for k, v in sample_batch.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(f\"{k}: {v.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define metrics model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefensiveMetricsPredictor(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_positions: int = 9,\n",
    "                 n_metrics: int = 4,\n",
    "                 d_model: int = 128,\n",
    "                 n_heads: int = 8,\n",
    "                 n_layers: int = 4,\n",
    "                 d_ff: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Position embeddings\n",
    "        self.position_embeddings = nn.Parameter(torch.randn(n_positions, d_model))\n",
    "        \n",
    "        # Metric embeddings\n",
    "        self.metric_embeddings = nn.Parameter(torch.randn(n_metrics, d_model))\n",
    "        \n",
    "        # Input projection for each metric type\n",
    "        self.metric_projections = nn.ModuleList([\n",
    "            nn.Linear(1, d_model) for _ in range(n_metrics)\n",
    "        ])\n",
    "        \n",
    "        # Age embedding\n",
    "        self.age_embedding = nn.Sequential(\n",
    "            nn.Linear(1, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, d_model)\n",
    "        )\n",
    "        \n",
    "        # Position-wise feed-forward\n",
    "        self.position_ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        \n",
    "        # Cross-position attention layers\n",
    "        self.cross_position_layers = nn.ModuleList([\n",
    "            nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Temporal attention layers\n",
    "        self.temporal_layers = nn.ModuleList([\n",
    "            nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Layer norms\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Output projections for each metric\n",
    "        self.metric_predictions = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(d_model, d_model),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(d_model, 1)\n",
    "            ) for _ in range(n_metrics)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor, age: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, seq_len, n_positions, n_metrics]\n",
    "            mask: [batch_size, seq_len, n_positions]\n",
    "            age: [batch_size, seq_len]\n",
    "        Returns:\n",
    "            output: [batch_size, n_positions, n_metrics]\n",
    "        \"\"\"\n",
    "        # Add shape assertions\n",
    "        batch_size, seq_len, n_positions, n_metrics = x.shape\n",
    "        assert seq_len == 3, f\"Expected seq_length 3, got {seq_len}\"\n",
    "        assert n_positions == 9, f\"Expected 9 positions, got {n_positions}\"\n",
    "        assert n_metrics == 4, f\"Expected 4 metrics, got {n_metrics}\"\n",
    "    \n",
    "        \n",
    "        # Process each metric separately and combine\n",
    "        metric_encodings = []\n",
    "        for i in range(n_metrics):\n",
    "            metric_data = x[..., i:i+1]  # [batch, seq, pos, 1]\n",
    "            metric_proj = self.metric_projections[i](metric_data)  # [batch, seq, pos, d_model]\n",
    "            metric_encodings.append(metric_proj)\n",
    "        \n",
    "        # Combine metric encodings\n",
    "        position_states = torch.stack(metric_encodings, dim=-2)  # [batch, seq, pos, n_metrics, d_model]\n",
    "        position_states = position_states.mean(dim=-2)  # [batch, seq, pos, d_model]\n",
    "        \n",
    "        # Add position embeddings\n",
    "        position_states = position_states + self.position_embeddings.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Process age information\n",
    "        age_embed = self.age_embedding(age.unsqueeze(-1))  # [batch, seq, d_model]\n",
    "        \n",
    "        # Cross-position attention\n",
    "        for layer in self.cross_position_layers:\n",
    "            # Reshape for attention\n",
    "            states = position_states.view(batch_size * seq_len, n_positions, -1)\n",
    "            states = layer(states, states, states, \n",
    "                         key_padding_mask=mask.view(-1, n_positions))[0]\n",
    "            position_states = states.view(batch_size, seq_len, n_positions, -1)\n",
    "        \n",
    "        # Temporal attention with age awareness\n",
    "        position_states = position_states.transpose(1, 2)  # [batch, pos, seq, d_model]\n",
    "        for layer in self.temporal_layers:\n",
    "            temporal_states = []\n",
    "            for pos in range(n_positions):\n",
    "                pos_states = position_states[:, pos]  # [batch, seq, d_model]\n",
    "                pos_states = pos_states + age_embed  # Add age information\n",
    "                pos_states = layer(pos_states, pos_states, pos_states)[0]\n",
    "                temporal_states.append(pos_states)\n",
    "            position_states = torch.stack(temporal_states, dim=1)\n",
    "        \n",
    "        predictions = []\n",
    "        for i in range(n_metrics):\n",
    "            metric_pred = self.metric_predictions[i](position_states[:, :, -1])  # [batch, n_positions]\n",
    "            predictions.append(metric_pred.unsqueeze(-1))  # [batch, n_positions, 1]\n",
    "        \n",
    "        # Stack along last dimension to get [batch, n_positions, n_metrics]\n",
    "        output = torch.cat(predictions, dim=-1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define positional predictor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionTransitionModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_positions: int = 9,\n",
    "                 n_metrics: int = 4,\n",
    "                 d_model: int = 128,\n",
    "                 n_heads: int = 8,\n",
    "                 n_layers: int = 4,\n",
    "                 d_ff: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Position hierarchy embeddings (e.g., SS->2B more likely than SS->1B)\n",
    "        self.position_hierarchy = nn.Parameter(torch.randn(n_positions, n_positions))\n",
    "        \n",
    "        # Position embeddings\n",
    "        self.position_embeddings = nn.Parameter(torch.randn(n_positions, d_model))\n",
    "        \n",
    "        # Age-based transition embeddings\n",
    "        self.age_transition = nn.Sequential(\n",
    "            nn.Linear(1, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, n_positions * n_positions)\n",
    "        )\n",
    "        \n",
    "        # Metric encoders\n",
    "        self.metric_encoders = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(1, d_model),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ) for _ in range(n_metrics)\n",
    "        ])\n",
    "        \n",
    "        # Position-specific metric attention\n",
    "        self.metric_attention = nn.MultiheadAttention(\n",
    "            d_model, n_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Historical position encoding\n",
    "        self.history_encoder = nn.GRU(\n",
    "            input_size=n_positions,\n",
    "            hidden_size=d_model,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Position transition layers\n",
    "        self.transition_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=d_model,\n",
    "                nhead=n_heads,\n",
    "                dim_feedforward=d_ff,\n",
    "                dropout=dropout,\n",
    "                batch_first=True\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.position_predictor = nn.Sequential(\n",
    "            nn.Linear(d_model * 3, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, n_positions)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, \n",
    "                defensive_metrics: torch.Tensor,  # [batch, n_positions, n_metrics]\n",
    "                position_history: torch.Tensor,   # [batch, seq_len, n_positions]\n",
    "                age: torch.Tensor,               # [batch, 1]\n",
    "                current_position: torch.Tensor    # [batch, n_positions] (one-hot)\n",
    "                ) -> torch.Tensor:\n",
    "        \n",
    "        batch_size = defensive_metrics.shape[0]\n",
    "        \n",
    "        # 1. Process defensive metrics\n",
    "        metric_embeddings = []\n",
    "        for i in range(defensive_metrics.shape[-1]):\n",
    "            metric = defensive_metrics[..., i:i+1]\n",
    "            metric_embed = self.metric_encoders[i](metric)\n",
    "            metric_embeddings.append(metric_embed)\n",
    "        \n",
    "        metric_tensor = torch.stack(metric_embeddings, dim=-2)\n",
    "        \n",
    "        # 2. Process position history\n",
    "        history_encoding, _ = self.history_encoder(position_history)\n",
    "        last_state = history_encoding[:, -1]\n",
    "        \n",
    "        # 3. Age-based transition probabilities\n",
    "        age_transitions = self.age_transition(age)\n",
    "        age_transitions = age_transitions.view(batch_size, self.position_hierarchy.shape[0], -1)\n",
    "        \n",
    "        # 4. Current position context\n",
    "        current_embed = torch.matmul(current_position, self.position_embeddings)\n",
    "        \n",
    "        # 5. Apply metric attention for each position\n",
    "        position_metrics = []\n",
    "        for pos in range(defensive_metrics.shape[1]):\n",
    "            pos_embed = self.position_embeddings[pos:pos+1].expand(batch_size, -1)\n",
    "            attended_metrics, _ = self.metric_attention(\n",
    "                pos_embed.unsqueeze(1),\n",
    "                metric_tensor,\n",
    "                metric_tensor\n",
    "            )\n",
    "            position_metrics.append(attended_metrics.squeeze(1))\n",
    "        position_metrics = torch.stack(position_metrics, dim=1)\n",
    "        \n",
    "        # 6. Combine all information\n",
    "        combined_state = torch.cat([\n",
    "            current_embed,\n",
    "            last_state,\n",
    "            position_metrics.mean(dim=1)\n",
    "        ], dim=-1)\n",
    "        \n",
    "        # 7. Generate transition probabilities\n",
    "        logits = self.position_predictor(combined_state)\n",
    "        \n",
    "        # 8. Apply position hierarchy constraints\n",
    "        hierarchy_weights = F.softmax(self.position_hierarchy, dim=-1)\n",
    "        current_pos_idx = current_position.argmax(dim=-1)\n",
    "        transition_constraints = hierarchy_weights[current_pos_idx]\n",
    "        \n",
    "        # 9. Final position probabilities\n",
    "        position_probs = F.softmax(logits * transition_constraints, dim=-1)\n",
    "        \n",
    "        return position_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our custom loss function for defense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefensiveMetricsLoss(nn.Module):\n",
    "    def __init__(self, metric_weights: Optional[Dict[str, float]] = None):\n",
    "        super().__init__()\n",
    "        self.metric_weights = metric_weights or {\n",
    "            'inn': 0.5,\n",
    "            'drs': 1.0,\n",
    "            'uzr': 1.0,\n",
    "            'oaa': 1.0\n",
    "        }\n",
    "\n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pred: [batch_size, n_positions, n_metrics]\n",
    "            target: [batch_size, n_positions, n_metrics]\n",
    "            mask: [batch_size, n_positions]\n",
    "        \"\"\"\n",
    "        # Add assertions to catch shape mismatches\n",
    "        assert pred.dim() == 3, f\"Expected 3D tensor, got shape {pred.shape}\"\n",
    "        assert pred.shape == target.shape, f\"Shape mismatch: pred {pred.shape} vs target {target.shape}\"\n",
    "        assert mask.shape == pred.shape[:2], f\"Mask shape {mask.shape} doesn't match pred {pred.shape[:2]}\"\n",
    "        \n",
    "        # Expand mask for broadcasting\n",
    "        mask = mask.unsqueeze(-1)  # [batch_size, n_positions, 1]\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        total_weight = sum(self.metric_weights.values())\n",
    "        \n",
    "        for i, weight in enumerate(self.metric_weights.values()):\n",
    "            # Calculate squared error for this metric\n",
    "            diff = (pred[..., i] - target[..., i]) ** 2  # [batch_size, n_positions]\n",
    "            \n",
    "            # Apply mask and normalize by number of valid positions\n",
    "            masked_diff = diff * mask.squeeze(-1)\n",
    "            valid_positions = mask.squeeze(-1).sum(dim=1) + 1e-8  # [batch_size]\n",
    "            \n",
    "            # Calculate mean loss per batch item\n",
    "            metric_loss = (masked_diff.sum(dim=1) / valid_positions).mean()\n",
    "            \n",
    "            # Add weighted contribution\n",
    "            total_loss += (weight / total_weight) * metric_loss\n",
    "        \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionTransitionLoss(nn.Module):\n",
    "    def __init__(self, position_weights: Optional[torch.Tensor] = None):\n",
    "        super().__init__()\n",
    "        self.position_weights = position_weights\n",
    "        \n",
    "    def forward(self, pred_probs: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pred_probs: [batch, n_positions] - Predicted position probabilities\n",
    "            target: [batch, n_positions] - One-hot encoded target positions\n",
    "        \"\"\"\n",
    "        # Cross entropy with position weights if provided\n",
    "        if self.position_weights is not None:\n",
    "            loss = F.cross_entropy(pred_probs, target.argmax(dim=-1), \n",
    "                                 weight=self.position_weights)\n",
    "        else:\n",
    "            loss = F.cross_entropy(pred_probs, target.argmax(dim=-1))\n",
    "        \n",
    "        # Add regularization to prevent overconfident predictions\n",
    "        entropy_reg = -torch.mean(torch.sum(pred_probs * torch.log(pred_probs + 1e-8), dim=-1))\n",
    "        \n",
    "        return loss - 0.1 * entropy_reg  # Small entropy regularization weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        valid_loader: DataLoader,\n",
    "        criterion: nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        scheduler: torch.optim.lr_scheduler._LRScheduler,\n",
    "        device: torch.device,\n",
    "        patience: int = 10,\n",
    "        max_grad_norm: float = 1.0,\n",
    "        checkpoint_dir: str = 'checkpoints'\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.device = device\n",
    "        self.patience = patience\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        \n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "    def train_epoch(self) -> float:\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        with tqdm(self.train_loader, desc='Training') as pbar:\n",
    "            for x, mask, weights, y in pbar:\n",
    "                x = x.to(self.device)\n",
    "                mask = mask.to(self.device)\n",
    "                weights = weights.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(x, mask, weights)\n",
    "                loss = self.criterion(output, y, weights)\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                pbar.set_postfix({'loss': loss.item()})\n",
    "                \n",
    "        return total_loss / len(self.train_loader)\n",
    "    \n",
    "    def validate(self) -> float:\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.valid_loader:\n",
    "                # Unpack position-specific data correctly\n",
    "                history = batch['history'].to(self.device)\n",
    "                history_mask = batch['history_mask'].to(self.device)\n",
    "                target = batch['target'].to(self.device)\n",
    "                target_mask = batch['target_mask'].to(self.device)\n",
    "                age = batch['age'].to(self.device)\n",
    "                \n",
    "                output = self.model(history, history_mask, age)\n",
    "                loss = self.criterion(output, target, target_mask)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "        return total_loss / len(self.valid_loader)\n",
    "    \n",
    "    def train(self, num_epochs: int):\n",
    "        best_valid_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = self.train_epoch()\n",
    "            valid_loss = self.validate()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            logger.info(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "            logger.info(f'Train Loss: {train_loss:.4f}')\n",
    "            logger.info(f'Valid Loss: {valid_loss:.4f}')\n",
    "            \n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                patience_counter = 0\n",
    "                self.save_checkpoint(epoch, valid_loss)\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            if patience_counter >= self.patience:\n",
    "                logger.info(f'Early stopping triggered after {epoch+1} epochs')\n",
    "                break\n",
    "                \n",
    "    def save_checkpoint(self, epoch: int, valid_loss: float):\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'valid_loss': valid_loss\n",
    "        }\n",
    "        path = os.path.join(self.checkpoint_dir, f'model_epoch_{epoch}_loss_{valid_loss:.4f}.pt')\n",
    "        torch.save(checkpoint, path)\n",
    "        logger.info(f'Checkpoint saved: {path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefensiveMetricsTrainer(Trainer):\n",
    "    def train_epoch(self) -> float:\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        with tqdm(self.train_loader, desc='Training Metrics') as pbar:\n",
    "            for batch in pbar:\n",
    "                # Unpack position-specific data\n",
    "                history = batch['history'].to(self.device)\n",
    "                history_mask = batch['history_mask'].to(self.device)\n",
    "                target = batch['target'].to(self.device)\n",
    "                target_mask = batch['target_mask'].to(self.device)\n",
    "                age = batch['age'].to(self.device)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(history, history_mask, age)\n",
    "                loss = self.criterion(output, target, target_mask)\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                pbar.set_postfix({'loss': loss.item()})\n",
    "                \n",
    "        return total_loss / len(self.train_loader)\n",
    "\n",
    "class PositionTransitionTrainer(Trainer):\n",
    "    def __init__(self, metrics_model: nn.Module, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.metrics_model = metrics_model  # Store metrics model\n",
    "    def train_epoch(self) -> float:\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        with tqdm(self.train_loader, desc='Training Position') as pbar:\n",
    "            for batch in pbar:\n",
    "                # Get defensive metrics predictions first\n",
    "                with torch.no_grad():\n",
    "                    defensive_metrics = self.metrics_model(\n",
    "                        batch['history'].to(self.device),\n",
    "                        batch['history_mask'].to(self.device),\n",
    "                        batch['age'].to(self.device)\n",
    "                    )\n",
    "                \n",
    "                # Prepare position transition inputs\n",
    "                position_history = batch['history_mask'].to(self.device)  # Use mask as one-hot position encoding\n",
    "                current_position = position_history[:, -1]  # Last position in sequence\n",
    "                age = batch['age'][:, -1].unsqueeze(-1).to(self.device)  # Current age\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                position_probs = self.model(\n",
    "                    defensive_metrics=defensive_metrics,\n",
    "                    position_history=position_history,\n",
    "                    age=age,\n",
    "                    current_position=current_position\n",
    "                )\n",
    "                \n",
    "                # Target is next season's position\n",
    "                target_position = batch['target_mask'].to(self.device)\n",
    "                loss = self.criterion(position_probs, target_position)\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                pbar.set_postfix({'loss': loss.item()})\n",
    "                \n",
    "        return total_loss / len(self.train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Training Defensive Metrics Model...\n",
      "Training Metrics:   0%|          | 0/22 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Expected 3D tensor, got shape torch.Size([32, 9, 1, 4])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 65\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m metrics_model, transition_model\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Train both models\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m metrics_model, transition_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 56\u001b[0m, in \u001b[0;36mtrain_models\u001b[1;34m(num_epochs)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_models\u001b[39m(num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m     55\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Defensive Metrics Model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m     \u001b[43mmetrics_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining Position Transition Model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     59\u001b[0m     metrics_model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Freeze metrics model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 76\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, num_epochs)\u001b[0m\n\u001b[0;32m     73\u001b[0m patience_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 76\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate()\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m, in \u001b[0;36mDefensiveMetricsTrainer.train_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     16\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(history, history_mask, age)\n\u001b[1;32m---> 17\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     20\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_grad_norm)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 19\u001b[0m, in \u001b[0;36mDefensiveMetricsLoss.forward\u001b[1;34m(self, pred, target, mask)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    pred: [batch_size, n_positions, n_metrics]\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m    target: [batch_size, n_positions, n_metrics]\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    mask: [batch_size, n_positions]\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Add assertions to catch shape mismatches\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m pred\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 3D tensor, got shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m pred\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m target\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape mismatch: pred \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m vs target \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m mask\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m pred\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMask shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmask\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match pred \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Expected 3D tensor, got shape torch.Size([32, 9, 1, 4])"
     ]
    }
   ],
   "source": [
    "# Initialize models and optimizers\n",
    "metrics_model = DefensiveMetricsPredictor().to(device)\n",
    "transition_model = PositionTransitionModel().to(device)\n",
    "\n",
    "# Initialize losses\n",
    "metrics_loss = DefensiveMetricsLoss()\n",
    "transition_loss = PositionTransitionLoss()\n",
    "\n",
    "# Initialize optimizers with different learning rates\n",
    "metrics_optimizer = optim.AdamW(metrics_model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "transition_optimizer = optim.AdamW(transition_model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "# Initialize schedulers\n",
    "metrics_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    metrics_optimizer, \n",
    "    T_0=10, \n",
    "    T_mult=2\n",
    ")\n",
    "transition_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    transition_optimizer, \n",
    "    T_0=10, \n",
    "    T_mult=2\n",
    ")\n",
    "\n",
    "# Initialize trainers\n",
    "metrics_trainer = DefensiveMetricsTrainer(\n",
    "    model=metrics_model,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    criterion=metrics_loss,\n",
    "    optimizer=metrics_optimizer,\n",
    "    scheduler=metrics_scheduler,\n",
    "    device=device,\n",
    "    patience=10,\n",
    "    max_grad_norm=1.0,\n",
    "    checkpoint_dir='checkpoints/metrics'\n",
    ")\n",
    "\n",
    "transition_trainer = PositionTransitionTrainer(\n",
    "    model=transition_model,\n",
    "    metrics_model=metrics_model,  # Pass metrics model for predictions\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    criterion=transition_loss,\n",
    "    optimizer=transition_optimizer,\n",
    "    scheduler=transition_scheduler,\n",
    "    device=device,\n",
    "    patience=15,  # Longer patience for position model\n",
    "    max_grad_norm=0.5,\n",
    "    checkpoint_dir='checkpoints/transition'\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "def train_models(num_epochs=100):\n",
    "    logger.info(\"Training Defensive Metrics Model...\")\n",
    "    metrics_trainer.train(num_epochs)\n",
    "    \n",
    "    logger.info(\"\\nTraining Position Transition Model...\")\n",
    "    metrics_model.eval()  # Freeze metrics model\n",
    "    transition_trainer.train(num_epochs)\n",
    "    \n",
    "    return metrics_model, transition_model\n",
    "\n",
    "# Train both models\n",
    "metrics_model, transition_model = train_models(num_epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
